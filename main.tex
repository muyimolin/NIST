%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 11 pt, onecolumn]{article}

\linespread{0.91}
\usepackage{tgpagella}
\usepackage{url}
\usepackage{hyperref}

\usepackage[dvips]{color}

\usepackage{pslatex}
% \usepackage{nopageno}
\usepackage{enumitem}
\setlist{itemsep=0em, topsep=0em}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}

\usepackage{tabularx}
%\usepackage{hangcaption}
\usepackage[font=footnotesize]{caption}
\usepackage[final]{graphicx}
%\DeclareGraphicsExtensions{.eps,.ps,.PS,.EPS}
\usepackage{epsfig}
%\usepackage{subfig}
\usepackage{tikz}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}

%\usepackage{xspace}
\usepackage{cite}
\usepackage{url}
\usepackage[letterpaper]{}
\usepackage{epstopdf}
\usepackage{authblk}
\usepackage{lineno}
\usepackage{todonotes}
\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
%\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{pgfgantt}
\usepackage[numbers,sort&compress]{natbib}

\usepackage[compact]{titlesec}
\usepackage{subcaption}

\usepackage{titling}
\setlength{\droptitle}{-5em}   % This is your set screw
\setlength{\belowcaptionskip}{-10pt}
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{0.4\baselineskip}{0.4\baselineskip}
\titlespacing*{\subsection}{0pt}{0.4\baselineskip}{0.4\baselineskip}
\titlespacing*{\subsubsection}{0pt}{0.4\baselineskip}{0.4\baselineskip}
\setlist[itemize]{leftmargin=*}

\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}
\renewcommand{\textfraction}{0.1}
\setlength{\floatsep}{5pt plus 2pt minus 2pt}
\setlength{\textfloatsep}{5pt plus 2pt minus 2pt}
\setlength{\intextsep}{5pt plus 2pt minus 2pt}

% for comments
\newcommand{\zhi}[1]{\textcolor{blue}{ZL: #1}}
\newcommand{\jie}[1]{\textcolor{green}{JF: #1}}
\newcommand{\brian}[1]{\textcolor{magenta}{BZ: #1}}



% \setlist[enumerate]{leftmargin=*}

\titlespacing{\paragraph}{%
  0pt}{%              left margin
  0\baselineskip}{% space before (vertical)
  0.3em}%               space after (horizontal)

\titlespacing{\subsection}{%
  0em}{%              left margin
  0.15\baselineskip}{% space before (vertical)
  0.15\baselineskip}%               space after (horizontal)

% figures with text wrapped around
\usepackage{wrapfig}
\usepackage{titling}
\setlength{\droptitle}{-5em}   % This is your set screw
\setlength{\textfloatsep}{0.8\baselineskip plus 0.2\baselineskip minus 0.5\baselineskip}

\newcommand{\fig}[1]{Fig.~\ref{#1}}
\newcommand{\figs}[2]{Fig.~\ref{#1} to~\ref{#2}\xspace}
\newcommand{\figa}[2]{Fig.~\ref{#1} and~\ref{#2}\xspace}
\newcommand{\eq}[1]{Eq.~(\ref{#1})}
\newcommand{\eqs}[2]{Equations~(\ref{#1}) to~(\ref{#2})}
\newcommand{\eqa}[2]{Equations~(\ref{#1}) and~(\ref{#2})}
% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb} % assumes amsmath package installed

\pagestyle{plain}                                                      %%
%%%%%%%%%% EXACT 1in MARGINS %%%%%%%                                   %%
\setlength{\textwidth}{6.5in}     %%                                   %%
\setlength{\oddsidemargin}{0in}   %% (It is recommended that you       %%
\setlength{\evensidemargin}{0in}  %%  not change these parameters,     %%
\setlength{\textheight}{8.5in}    %%  at the risk of having your       %%
\setlength{\topmargin}{0in}       %%  proposal dismissed on the basis  %%
\setlength{\headheight}{0in}      %%  of incorrect formatting!!!)      %%
\setlength{\headsep}{0in}         %%                                   %%
\setlength{\footskip}{.5in}       %%                                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                   %%
\renewcommand{\refname}{References Cited}                              %%
\bibliographystyle{plain}



\begin{document}


\setcounter{page}{1}
% \input{Summary.tex}
\input{FW-HTF_Abstractv04.tex}
\pagebreak

% % \maketitle

% \begin{center}
% 	{\large \bf NRI: FND: COLLAB: Intelligent Mediating Robot --- Lifelong Learning and Fast Intent Forecasting in Simultaneous Multi-lateral Physical Human-robot Interaction}\\
%     \vspace{4pt}
% % 	\renewcommand{\baselinestretch}{1}
%    	PI: Zhi Li (Worcester Polytechnic Institute)\\
%     Co-PI: Brian Ziebart (University of Illinois at Chicago), Jie Fu (Worcester Polytechnic Institute)
% \end{center}

% \vspace{-4pt}

% % \hrule

% % \thispagestyle{empty}
% % \pagestyle{empty}


% \pagestyle{plain}
% \setcounter{page}{1}

% % % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \begin{abstract}

% % \noindent

% % \end{abstract}

% %-------------------------------------------------------------------------
% \section{Research Motivations and Objectives}\label{sec:intro}
% %-------------------------------------------------------------------------

% \paragraph*{Research Motivations:} 
% Teleoperated medical robots mediate interactions between their operators, on-site medical personnel, and patients.
% Ideally, they can provide greater safety and dexterity than autonomous robots, particularly for risk-sensitive tasks that require fine motor skills and contextual decision-making~\cite{taylor2016medical}.
% However, direct teleoperation demands significant physical and mental effort, particularly for novices.
% Our research seeks to reduce operation and learning effort for direct teleoperation by letting the robot learn while being operated by an expert and interacting with on-site nurses or patients. 
% By developing motion intelligence in the form of both high-level task knowledge and low-level motion skills, our proposed system will be able to provide adaptive assistance to novice teleoperators.
% To achieve this, we will establish an novel framework for \textbf{learning from robot-mediated multilateral physical interactions} that (1) integrates off-line and on-line learning of low-level motions skills and high-level task knowledge from demonstrations, and (2) uses this task knowledge to reason about its teleoperator's and human partner's intentions.
% Our research will improve the dexterity, customizability, and usability of mobile humanoid nursing robots in performing complex collaborative tasks, and will reduce the learning curve for novice teleoperators.
% Our proposed theory and methods for lifelong learning and fast intent inference will also broadly benefit shared-autonomous robots operating in human environments for medical, industrial and social applications. 

% % Teleoperated medical robots enable collaborations between remote and on-site medical personnel as well as robot-mediated interactions with patients. These teleoperated robots often provide for greater safety, dexterity and effectiveness than existing autonomous robots, particularly on risk-sensitive tasks that require fine motor skills and contextual decision-making~\cite{taylor2016medical}. 
% % However, direct teleoperation of mobile humanoid nursing robot demands significant physical and mental efforts, particularly for novices. Thus, we seek to reduce the operation and learning efforts of direct teleoperation by teaching the robot through simultaneous interactions with expert teleoperator and an on-site nurses or patient, and develop its motion intelligence which will further provide user-adaptive assistance to novice teleoperators.
% % % PM: This sentence (below) is a bit hard to understand 
% % In this proposal, we aim to establish a novel framework of \textbf{learning from robot-mediated multilateral physical interactions} that (1) integrates off-line and on-line learning of low-level motion skills and high-level task knowledge from demonstrations, and (2) uses this task knowledge to reason about human teleoperator and end user's intentions, preferences and collaborative nursing task decomposition. Our research will improve the dexterity, customizability, and usability of mobile humanoid nursing robots in performing complex collaborative tasks, and reduce the learning curve for novice users. Our proposed theory and methods for lifelong learning and fast intent inference will broadly benefit shared autonomous systems with blending control or switching control, and other teleoperated robots operating in human environments for medical, industrial and social applications. 

% \begin{wrapfigure}{r}{0.65\linewidth}
% \centering
% \vspace{0.8ex}
% \includegraphics[width=0.99\linewidth]{fig//DemoTasks}
% \caption{A teleoperated nursing robot can develop its motion intelligence by learning from direct robot-mediated interactions between (a) an experienced teleoperator and on-site patients and nurses. The tasks we focus on include: (b) moving a patient transfer bed; (c.1-2) organizing and cleaning the patient's room, which on-site nurses can perform more efficiently by collaborating with the teleoperated nursing robot; and (d.1-4) preparing and serving food to a patient, which involves interdependent and interactive task steps.}
% \label{Tasks}
% \vspace{1.4ex}
% \end{wrapfigure}

% \paragraph*{Motivating Nursing Tasks:} This project focuses on frequent nursing tasks that could benefit from \textit{robot-mediated collaborations} between a teleoperator and an on-site nurse, such as moving a patient transfer bed in a cluttered patient room (\fig{Tasks}(b)), organizing medical supplies (\fig{Tasks} (c.1)) and cleaning up debris (\fig{Tasks}(c.2)).
% \textit{Robot-mediated interactions} between a teleoperator and a patient, such as food preparation and serving, should also be enabled.
% Under direct teleoperation, these tasks are feasible for a mobile humanoid nursing robot, but task completion time is about 95x times slower than direct human operation~\cite{Hauser_Li_TRINA:17}.
% Despite bilateral telepresence, it is still difficult for teleoperators to communicate complex motion intents, which are essential for intimate collaboration.
% When telepresence communication is frequent, attention to maintaining video/audio communication channels may distract and slow down the teleoperators' control of the robot, adding to their perceptual burden~\cite{Hauser_Li_BiTelepresence:17}.
% So far, no data is available for the feasibility and performance of these tasks under fully autonomous control.
% However, research efforts on motion planning, learning and human-robot interaction have revealed many significant challenges in these areas.

% \paragraph*{Challenges:} This research aims to address three main challenges. \underline{\textbf{\textit{For the mediating robot}}}, {\it neither direct teleoperation nor fully-autonomous control are ideal options}; direct teleoperation often lacks precision for fine-grained control enabled by on-site sensing information ~\cite{chen2007human,chen2011supervisory}, while pure autonomy often cannot appropriately interpret highly collaborative and interactive tasks with human partners~\cite{beer2014toward,nahavandi2017trusted,alaieri2016ethical}. \underline{\textbf{\textit{For end users}}}, such as patients and on-site human nurses, {\it physical interactions and collaborations are difficult because robot autonomy may not appear natural and fluent}~\cite{goetz2003matching,strabala2013towards, mitsunaga2008adapting}. This unexpected robot behavior may confuse, surprise or even intimidate end users, adding to patient stress, discomfort and/or anxiety~\cite{nomura2008prediction}. \underline{\textbf{\textit{For the teleoperator}}}, controlling a mobile humanoid robot to perform dexterous coordinated motions involves a {\it significant learning curve for novice users to adapt to the low transparency of the teleoperation system and physical capabilities of the robot}~\cite{rosen2002task,ballantyne2002robotic,novick2003analysis,sudan2012multifactorial,sng2013multiphasic}. This prevents nurses, who have significant first-hand experience and task knowledge, from effectively teleoperating a different physical embodiment.
% In addition, perceiving the remote environment to control position and orientation precisely are difficult despite having many available static and moving camera views. Although novice teleoperators' high-level knowledge of task goals, procedures, and object affordances is useful, the low-level motion primitives and their coordination, as well as the motion-perception mapping have to be relearned through a  significant amount of practice. 

% \begin{wrapfigure}{r}{2.4in}
% \vspace{-0.5ex}
% \centering
%   \includegraphics[width=0.99\linewidth]{fig//001TeleNursing_Overview}
% \caption{Learning from robot-mediated multi-lateral physical interactions.}
% \label{001TeleNursing_Overview}
% \end{wrapfigure}

% \paragraph*{A Novel Framework for Robotic Mediation Between Teleoperators and End Users:} We propose to address the above challenges by leveraging the benefits of {\it human teleoperation primarily at higher-levels of task abstraction} and the benefits of {\it autonomous control primarily at lower-levels of task completion}. Shown in~\fig{001TeleNursing_Overview}, our approach is enabled by the key idea of {\bf observing how a tele-nursing robot equipped with a human mind (of an expert teleoperator) behaves when interacting with and collaborating with people.} In this learning context, the teleoperator and end user adapt to each other and establish a feasible and potentially optimal collaboration paradigm given the physical capability of the teleoperated robot. Their joint effort demonstrates to the mediating robot the big picture of task collaboration, and the role it can play as a competent participant. 
 
% Our proposed research aims to address the need for \textit{\textbf{developing contextual and user-adaptive robot motion intelligence}}.
% % PM: I changed "evolvable" to "adaptable" because "evolvable" has a technical meaning which I don't think you intend (evolutionary algorithms).
% Our preliminary work has evaluated the capability of a mobile humanoid nursing robot on frequently performed nursing tasks in a simulated patient room~\cite{Hauser_Li_TRINA:17}. Under direct teleoperation, this robot is capable of performing many nursing tasks that are still not feasible under autonomous control, yet the robot task completion times are far longer than those of human nurses. This is mostly due to the teleoperators' difficulties in: (1) learning perception mappings to infer spatial relationships from multiple camera views, and (2) learning motion mappings from an input device to the robot to perform precise motion control and motion coordination. Despite the perception and motion control challenges, human teleoperators are still superior in high-level task planning, task performance evaluation (whether the task has been performed satisfactorily), correction (updating plans based on current task status) and interactive response. A paradigm of shared-autonomous control will synergize the human capability for decision making, task performance evaluation, and correction, with the robot's accurate perception and motion control. A tele-nursing robot capable of lifelong learning and fast intent forecasting will be able to develop contextual motion intelligence through robot-mediated interactions between expert teleopertors and end users, evolve its motion intelligence accordingly to new task demonstrations and user preferences, and provide user-adaptive assistance to novice teleoperator. We envision our research efforts will \textbf{improve the customizability} of nursing robots, and \textbf{lower the barriers} for medical personnels to use tele-nursing technologies. 

% \paragraph*{Activities:} Our proposed project involves three research activities: 
% \begin{itemize}
% \item \textbf{Activity 1} --- Enable the robot to acquire contextual and customizable task knowledge and motion skills through lifelong learning from collaborative multi-lateral physical human-robot interaction tasks. 
% \end{itemize}
% \noindent
% % PM: Don't forget these missing citations!
% Our proposed \textbf{lifelong robot learning approach} aims to overcome the limitations of motion planning~\cite{HandbookOfRobotics_motionplanning} and robot learning methods~\cite{HandbookOfRobotics_learningfromhuman,HandbookOfRobotics_robotlearning} by integrating their respective advantages into a framework for multi-lateral physical human-robot interactions/collaboration. In this framework, a mediating robot observes the contextual interactions between an expert teleoperator and end users, and continuously acquires and updates its high-level task knowledge and low-level motion skills. At a high level, the robot can learn abstract representations of task structure and probabilistic models that relate actions and effects~\cite{konidaris2018skills}. It can also learn the object affordance for individual and joint actions~\cite{konidaris2012robot,niekum2013semantically,knoblich2011psychological}. At low levels, the mediating robot can learn parameterized motion skills such as motion primitives and their temporal-spatial coordination~\cite{ijspeert2013dynamical,calinon2010learning,meier2016probabilistic,maeda2017phase}. Such motion skills can be directly used for motion reproduction, since they include the effects of robot physical embodiment and the transparency of the teleoperation interface, which cannot be learned from observing human-human interactions. Our approach also naturally addresses co-adaptive phase matching between humans and the robot and therefore differs from robot learning without human partners. More importantly, we integrate both on-line and off-line learning so that robot behavior can be customized with minimal human teaching effort. Fast incremental learning will be developed for the robot to achieve robust yet adaptive motions for personalized human-robot interaction.
% %To account for both behavior robustness and fast adaptation, our method will compare new demonstrations with the existing models of task representation and motion skills. Fast incremental learning will be performed if no significant differences are observed. Otherwise, the robot will create special motion skill models after confirming with a human expert. 

% \begin{itemize}
% \item \textbf{Activity 2} --- Develop fast intention forecasting approaches for user-adaptive task-sharing, joint performance optimization, real-time motion prediction and recognition. 
% % PM: Changed 'recognition' from 'recognization' which I don't think is a word? Change back if it's jargon I'm not familiar with.
% \end{itemize}

% % PM: Putting this flush with the bottom of this paragraph would be better, but I couldn't figure out how to do that easily. The old placement caused some unfortunate justification issues, which is why I moved it at all.
% \begin{wrapfigure}{r}{3.5in}
%   \includegraphics[width=0.99\linewidth]{fig//TRINA_system}
%     \caption{(Left) The Tele-robotic Intelligent Nursing Assistant (TRINA) system and (Right) its operator console which can support direct teleoperation and shared-autonomous control.}
%     \label{fig:Trina}
% \vspace{1.0ex}
% \end{wrapfigure}
% \noindent
% The \textbf{systematic approach for fast intent inference} we propose enables learning the rewards and constraints that govern (1) motion coordination under shared autonomous control, and (2) dynamic task-sharing of robot-mediated nursing tasks that result from the explicit negotiation and implicit adaptation between a teleoperator and end user. In addition,  the robot will learn \textit{motion coordination strategies} to facilitate teleoperation. For example, the robot will be equipped with knowledge of how to autonomously move the mobile base to facilitate manipulation when moving a patient transfer bed with a human nurse. It can also learn \textit{collaboration strategies}, such as where to grasp on the patient transfer bed given where the human nurse grasps, and how to push the patient transfer bed to assist the human nurse's motion. Besides inferring task rewards and constraints, we are also interested in real-time motion prediction using inference about an end user's intent, so that the robot can behave pro-actively and naturally in interactions with the end user. To assist novice teleoperators in accurate motion control, we propose an \textit{active intent detection method} that infers the object a novice intends to operate, and autonomously controls low-level motions based on an object's affordances.

% \begin{itemize}
% \item \textbf{Activity 3} --- Implement lifelong robot learning and fast intent inference on the Tele-robotic Intelligent Nursing Assistant (TRINA) system, and systematically evaluate their performance on assisting novice teleoperators in robot-mediated interactions/collaborations. 

% \end{itemize}

% \noindent
% We plan to implement our proposed methods for lifelong learning and fast intent forecasting on a mobile humanoid nursing robot platform (\fig{fig:Trina}) and validate their efficiency through in-lab testing with three collaborative and interactive nursing tasks (\fig{Tasks}). With the support of NSF grants (NSF RAPID \#IIS-1513221 and NSF CAREER \#1253553), our previous work developed this Tele-robotic Intelligent Nursing Assistant (TRINA) system to minimize the risks of exposure to highly infectious diseases among healthcare workers like nurses, Emergency Medical Technicians (EMTs), and aid workers during outbreaks (e.g., Ebola and Zika epidemics of 2014--2016). This nursing robot platform consists of two compliant 7 DoF arms on a humanoid torso (Rethink Robotics Baxter), an omnidirectional mobile base for indoor navigation (HStar AMP-I), and three-fingered grippers (Righthand Robotics ReFlex grippers) on each hand to enable manipulation. It is also equipped with a variety of sensors for visual feedback and environment mapping, and devices for bidirectional telepresence. Under direct teleoperation, this nursing robot can act as a surrogate for medical personnel to perform frequent light- to medium-duty caregiving tasks in quarantine environments~\cite{Hauser_Li_TRINA:17}. More importantly, the development of tele-nursing approaches promises a future that leverages robots to enable remote medical personnel to improve the quality, availability, and affordability of healthcare. Through user studies we will evaluate if the system can (1) efficiently acquire task knowledge and motion skills from experts, (2) provide user-adaptive assistance in robot-mediated nursing tasks, and (3) reduce the learning and operation barriers for novice teleoperators. 


% % PM: Should this be "Intellectual Merit"?
% \paragraph*{Intellectual Merit:} Our project aims to develop fundamental theory and methods that enable \textit{\textbf{contextual and user-adaptive motion intelligence for robotic systems}}, and implement them in a nursing robot scenario. The proposed contribution will be threefold: (1) We will construct a hierarchy of contextual motion intelligence by unifying several learning paradigms. Specifically, we will generalize skill trees \cite{konidaris2012robot} and grammatical inference \cite{de2010grammatical} (a.k.a. Automata learning) as abstraction-based learning methods for task constraints and decomposition, and develop methods for learning interactive dynamic motion primitives for low-level planning and control to achieve natural and intuitive robotic behaviors. (2) We will combine inverse reinforcement learning ---an intent inference mechanism---with cooperative game theory to develop shared intent inference algorithms for human-robot collaborative tasks. (3) We will develop algorithms for online behavior adaptation and fast intent forecasting, and propose methods to construct assistive navigation and manipulation modules in teleoperation. Our proposed approaches are generally applicable for robot learning and user adaptation of tele-robotic systems. They will provide insights and guidance for evolving directly teleoperated robots to intelligent shared-autonomous robots, and reduce learning and operation efforts for tele-robotic systems.

% % \todo{JF: the last sentence may remove nursing totally. as this is general scentific contribution. }

% \paragraph*{PI's Qualifications:}
% PI Zhi Li has significant experience in motion planning and learning for human-compatible and human-like medical robots, including exoskeletons for rehabilitation, arm-like surgical robots and nursing robots, and has also worked on mechanical design optimization~\cite{li2016design}, teleoperation and haptics~\cite{li2009networked,li2009remote}, and physical human-robot interaction~\cite{Hauser_Li_BiTelepresence:17}. Related to this proposed project, she has studied inferring the reward function that governs
% % PM: Removed "reward function that governs" to make this section read smoother and save space.
% the regularity and variability of reaching and reach-to-grasp motions in a 3D workspace, and developed mathematical models that can accurately predict human arm postures in real-time~\cite{kim2012resolving,Rosen_Li_EMBC:13,Rosen_Li_IROSChapt:13,Rosen_Li_IROS:14,Rosen_Li_J:14, li2017reaching}. Li's motion learning and prediction algorithm can resolve the kinematic redundancy of a dual-arm rehabilitation exoskeleton, enable it to render arm postures that are fully compatible with healthy operators, and correct abnormal joint coordination due to motor disability. She also developed the Tele-robotic Intelligent Nursing Assistant (TRINA) system, and conducted system capability evaluation under direct teleoperation~\cite{Hauser_Li_TRINA:17}.

% PI Jie Fu's research focuses on planning and control synthesis of verifiable robotic systems. Her thesis work developed a symbolic method for abstraction-based control of hybrid systems that enables scalable planning for complex behaviors under temporally and spatially dependent tasks \cite{fu2013bottom}. Fu introduces grammatical inference \cite{fu2014adaptive} as a behavior inference method to enable adaptive symbolic planning in task space. In recent work, Fu developed a  planning method for a class of stochastic shared autonomous systems, featuring  human-robot task sharing \cite{fu2016synthesis}. This research allows the system to achieve Pareto-optimality for system  performance while balancing against human efforts. Other related  work  includes game theoretic planning for multi-agent systems under  temporal logic tasks \cite{fu2015concurrent} and approximate optimal control under temporal logic constraints, which include common temporally extended goals \cite{fu2017sampling}.

% PI Brian Ziebart is an expert in structured prediction methods for sequential behavior.  Ziebart developed maximum entropy inverse reinforcement learning (MaxEnt IRL) \cite{ziebart2008maximum,ziebart2010modeling} to learn the utilities motivating demonstrated behaviors while simultaneously providing robust prediction guarantees. 
% Ziebart's projects based on these methods have enabled more accurate predictions of user intent in pointing tasks for graphical user interfaces \cite{ziebart2012probabilistic}, physical motions \cite{monfort2015intent}, and teleoperation \cite{schultz2017goal}.
% Recently, Ziebart has developed methods for accelerating predictions of these methods using ideas from heuristic-guided search \cite{monfort2015softstar} and hybrid decompositions of the state space \cite{byravan2015graph}.


% %-------------------------------------------------------------------------
% \section{Related Work}\label{sec:related}
% %-------------------------------------------------------------------------

% % PM: I haven't gone over these commented sections. Please let me know if I should.

% % %-------------------------------------------------------------------------
% % \subsection{Background --- Toward a robotic nursing assistant of motion intelligence}\label{sec:related-TRINA}
% % %-------------------------------------------------------------------------


% % \noindent
% % The rapid development of telerobotic approaches promises a future that synergizes the capabilities of robots and remote medical personnel to improve the quality, availability, and affordability of healthcare. For example, these technologies minimize the risks of exposure to highly infectious diseases for  healthcare workers like nurses, Emergency Medical Technicians (EMTs), and aid workers during outbreaks, such as the Ebola and Zika epidemics of 2014--2016. With the support of NSF grants (NSF RAPID \#IIS-1513221 and NSF CAREER \#1253553), our previous work developed the Tele-robotic Intelligent Nursing Assistant (TRINA) system to act under teleoperation as a surrogate for medical personnel to perform frequent light- to medium-duty caregiving tasks in quarantine environments~\cite{Hauser_Li_TRINA:17}.

% % \paragraph*{Platform} Shown in~\fig{fig:Trina}, this nursing robot consists of a dual-armed humanoid torso (Rethink Robotics Baxter), an omnidirectional mobile base (HStar AMP-I), and three-fingered grippers (Righthand Robotics ReFlex grippers) on each hand to enable manipulation and indoor navigation capabilities.  A telepresence screen, microphone, and speakers provide bidirectional audio/visual communication between operator, patients, and other healthcare workers at the remote site. The power cable roller has a 30-foot extension cord that retracts and locks at any length, providing a large enough range of motion to traverse a typical hospital room. A variety of sensors situated on the robot provide visual feedback, including the built-in ultrasonic rangefinders to detect people in the robot's vicinity, a Microsoft Kinect 2 attached to the robot's chest, two Intel RealSense F200 3D cameras attached to the robot's wrists, two Huokuyo LIDAR sensors attached to the mobile base. All of the RGB+D/LIDAR sensors contribute to building 3D maps of the environment, which are provided to the operator's console. Four computers are mounted on the robot's chassis, one for motion control and three for vision processing. 

% % \jie{include a description on how you want to improve the mechanical design of the precise positioning unit.}




% % and develop user-adaptive assistive modules to . For instance, a tele-nursing robot of such motion intelligence would rather interpret and process their operation of input devices as high-level motion intent (e.g., reaching towards an object), instead of execute as direct command (e.g., the robot end-effector trajectory). 


% % Direct teleoperation of a nursing robot demands so much perception and motion control efforts that a teleoperating nurse may not be able to spare attention to the communication with a patient, which is essential in patient-caring~\cite{Hauser_Li_BiTelepresence:17}. 

% % \paragraph*{A nursing of motion intelligence} Despite of the above perception and motion control challenges, 




% % Due to significant challenges perception and motion control, even an expert teleoperator teleoperated robot cannot behave fluently and naturally in terms of arm posture and the timing of its responses When interacting with a human end users. 






% % Here we present the perception and motion control challenges observed in the direct teleoperation of nursing tasks~\cite{li2017development}: 






% % \noindent
% % (1) Perception Challenge --- Human teleoperator perceives the task and environment primarily through robot cameras, and has deep learning curve for estimating the spatial relationship from fixed and/or moving camera view: fixed camera has limited vision range and perspective, while moving camera poses challenges in estimating object spatial relationship, particularly for distance and orientation. On the other hand, the nursing robot on-site can observe the task directly and in close distance; It is also capable of accurately perceiving the spatial relation (e.g. object position, relative distance), and motions (e.g. the estimated speed of a moving object).

% % \noindent(2) Motion challenges --- Novice teleoperators have many difficulties in controlling the coordination of many degrees of freedom. It is hard to control dexterous motions that require coordination between two arms, between arm and hand, or between manipulator arms and mobile base navigation. In addition, It takes significant efforts to learn the mapping from input device operation and robot motions, particularly for precise control of  orientation. To novice teleoperator, controlling relative motion (moving left/right, further/close with respect to reference object) is easier than controlling absolute motion (moving to a specific x, y, z). They cannot distinguish/control motions that slightly differ in velocity and direction very well. Due to significant challenges in motion control, a teleoperated robot cannot behave fluently and naturally in terms of arm posture and the timing of its responses When interacting with a human end users. Direct teleoperation demands overwhelming control effort such that a teleoperating nurse do not have very much attention to communicate with a patient, which is an essential component of patient-caring. 



% % Therefore, the teleoperation system  The proposed user assistive module aims to combine the human capability of decision making and task performance evaluation, correction, with the robot's capability of perception and motion accuracy. To justify the design of user assistive module, here we present the perception and motion control challenges observed in the direct teleoperation of nursing tasks~\cite{li2017development}:

% % it is difficult for  it is still challenging for novice users to learn the spatial relationships of the workspace from a combination of static and moving camera views. 

% % Despite our efforts in developing an informative, versatile, and intuitive user interface, 



% % To address the above challenges, 

% % For instance, the teleoperation system would rather interpret and process their operation of input devices as high-level motion intent (e.g., reaching towards an object), instead of execute as direct command (e.g., the robot end-effector trajectory). The proposed user assistive module aims to  To justify the design of user assistive module, here we present the perception and motion control challenges observed in the direct teleoperation of nursing tasks~\cite{li2017development}: 
% % \jie{This problem formulation should be merged with task 2.}

% % It  the strength of human and robots in perception and motion control while compensating the weakness of each other. 

% % \jie{need a mathematical formulation. Related: intention-aware motion planning in mixed-observation MDP. However, the challenge is that novice users's demonstration can be errornous. Potentially we can propose to use expert demonstrations to infer the intention of novice users, based on human motion similarity. May relate to Co-active learning.}



% % \paragraph*{Lessons learned:} The limitations revealed in previous capability evaluations show that it is necessary to endow the nursing robot with motion intelligence to understand collaborative task decompositions and assignments, and to provide adaptive assistance based on the human teleoperator/partner's level of skill. In multi-lateral pHRI, collaborative task decomposition not only concern what operation a shared object can afford, but also the dynamic task-sharing between the teleoperated mediating robot and the human partner on-site. From expert teleoperator and end user, a mediating robot can observe their robot-mediated collaboration to acquire high-level models for the collaborative task decomposition, including the \textit{dynamic task-sharing} and \textit{shared object affordance}. It can also learn low-level models for movement primitives and their temporal-spatial coordination, to automate the precise low-level control based on the explicit and/or implicit user intentions.  

% % \jie{In this paragraph, we need to clarify which tasks are teleoperated. For example, dexterous in-hand manipulation may be autonomous while the user tele-operate macro-action. this is to remove the concern that we will track users' figure to perform tele-op of robot hand.}




% %-------------------------------------------------------------------------
% \subsection{Lifelong learning through human-robot interactions}\label{sec:related-learning}
% %-------------------------------------------------------------------------


% % \begin{figure}[h]
% % \begin{center}
% % \includegraphics[width=2in]{fig/trajectories.png}
% % \includegraphics[width=2in]{fig/learned_cost.png}
% % \includegraphics[width=2in]{fig/forecast.png}
% % \end{center}
% % \caption{(left) Users' trajectories within an environment. (center) A learned cost function explains trajectories as solutions to plans (red is high cost). (right) A forecast of future trajectories (red is most likely).}
% % \label{fig:forecast}
% % \end{figure}

% \noindent
% Programming robots to appropriately perform nursing tasks across a wide range of contexts is a daunting undertaking.  
% Automatically learning in a flexible manner is far more desirable.
% Lifelong learning for robotics \cite{thrun1995lifelong,thrun1995lifelong2,thrun1998lifelong} seeks to incrementally and continuously learn knowledge from some tasks that can be transferred to other tasks. 
% It has been more recently applied to the open-ended task of reading the web to collect relational facts \cite{banko2007strategies,carlson2010toward}. 
% However, formulations based on classical machine learning paradigms, though appealing in theory, are ill-suited.
% Reinforcement learning, for example, would require inordinate amounts of
% feedback (including feedback for negative outcomes)
% before appropriate behaviors could possibly be realized.
% To avoid this difficulty, we propose lifelong learning based on directly learning appropriate assistive behavior from the demonstrated robot-mediated interactions between human experts.
% Though many forms of imitation learning exist \cite{argall2009survey}, we focus on the following methods: (1) {\bf learning dynamic motion primitives} \cite{matsubara2011learning,kulic2012incremental} that can be adapted for new contexts, (2) \textbf{learning abstract task representations} for high-level task reasoning and generalization~\cite{niekum2013semantically,konidaris2012robot,konidaris2018skills}, (3) learning object affordances that constrain individual and joint actions~\cite{aksoy2011learning,knoblich2011psychological,shu2017learning}, and (4) {\bf inverse reinforcement learning} \cite{ng2000algorithms,abbeel2004apprenticeship}, which seeks utility functions that make demonstrations rational in sequential decision and control processes.

% %-------------------------------------------------------------------------
% \subsection{Intent forecasting in robot-mediated interactions}\label{sec:related-intent}
% %-------------------------------------------------------------------------

% %Intent inference endows robots with socially appropriate and natural-looking behaviors and improves the fluency of human-robot interactions~\cite{calinon2009learning,strabala2013towards}. 
% \noindent
% Instead of inferring intent from more explicit cues (e.g., gaze, body gestures~\cite{huang2016anticipatory}), inferring the implicit intent encoded in human motion may be more efficient and less intrusive, since it minimizes end user effort for maintaining explicit or even exaggerated human-robot communication. At a low-level, a robot can predict its human partner's motion based on the knowledge of tempo-spatial coordination observed in individual and interactive human motions~\cite{Hogan_Flash:85, sisbot2012human,huber2010assist, glasauer2010interacting, strabala2013towards, li2015rssworkshop, perez2015fast,maeda2017phase}. At a high-level, the intent inference problem can be formulated as inferring the parameters of a dynamic model~\cite{wang2013probabilistic}, Bayesian network~\cite{liu2016fuzzy} or Markov decision process~\cite{mcghan2015human} and tackled using techniques such as inverse linear-quadratic regulation (ILQR)~\cite{monfort2015intent} and approximate expectation-maximization~\cite{ravichandar2017human}. However, state-of-the-art methods for motion prediction and motion intent inference haven't fully considered that in a successful collaboration, \textit{the collaborative partners do not necessarily insist on their initial intents and plans, but will adjust based on explicitly negotiated task-sharing and/or inter-adaptation to each other's capabilities}~\cite{nikolaidis2017human}. In human-robot interactions, %it is quite usual that 
% humans often adapt too much to the robot because they do not trust the robot's task experience and decision-making intelligence. They are also not clear about what the robot can achieve given its physical capability and therefore make conservative assumptions for task sharing. Although the collaborative task succeeds, the robot may end up with little contribution towards their shared goal, and even perform below its potential~\cite{hancock2011meta,lee2013computationally,salem2015would}. \textit{Our proposed framework of learning from robot-mediated interactions provides an effective method to fix this problem}: An expert teleoperator will not only endow the mediating robot with human-level intelligence for decision-making, but can also investigate how to fully exploit its physical capabilities in interactive/ collaborative tasks. From the human partner's perspective, it is also convenient to negotiate with the teleoperator for optimal task-sharing, and establish an ideal collaboration paradigm for the mediating robot to perform the task under shared autonomy.  

% % It can also be modeled as a Markov decision process so that the human partners' future course of action based on the knowledge of the human partner's goal sequences, recent action history, and observed correlation between behaviors with actions~\cite{mcghan2015human}.



% %-------------------------------------------------------------------------

% % PM: Haven't reviewed this

% % The natural separation of macro- and micro-structures in human arms has inspired the development of robotic manipulators with macro- and micro-structures~\cite{Nagai:94,Murakami_Yoshikawa:93}. 
% % However, little work has been done to relate the control strategy for coordinating macro- and micro-structures with the motion studies on human arm-hand-finger coordination. For example, human motion studies have revealed that a reach-to-grasp motion can be constructed from a reaching motion toward the same target position. If the target orientation is perturbed when the hand is still approaching to the target, the hand path will remain approximately the same, while the hand orientation will be gradually turned from the one that matches the original target orientation to the one that matches the new target orientation~\cite{Tillery_Fan:06}. The PI's research on reach-to-grasp motions further pointed out that (1) the arm posture will adjust to accommodate the grasping hand pose, and (2) if there exist multiple DOFs that can be used to satisfy the same task specification (e.g., the grasping orientation in one dimension), the DOFs of the macro-structure will be largely unused until the DOFs of the micro-structure have been mostly exploited. On the other hand, redundancy resolution strategies have been proposed for flexible-macro/rigid-micro manipulators~\cite{Yoshikawa_Nakamura:87,lew1995micro}, compliant base manipulators~\cite{lew2001simple}, and coarse/fine dual-stage manipulators~\cite{kumar2000augmentation, kwon2001coarse,dong2008stiffness}, so that the macro- and micro-structures can compensate each other in terms of precision and motion range. The redundant DOFs can be used to resist perturbations/vibrations~\cite{lew1995micro, kwon2001coarse}, and they can also enable the micro-structure to satisfy additional task constraints or to maintain a desired level of manipulability~\cite{Yabuta_Quan:06,Yabuta_Huang:06,Yabuta_Huang:10}. These motion coordination strategies cannot render natural-looking motions and therefore cannot be directly applied to coordinating the arm and hand of humanoid and human compatible robots when human-like motion is desired. The existing research efforts are also limited by their focus on individual reaching or grasping tasks. It is unclear when and how to move which parts of the robot to better perform tasks in sequence, tasks in a group, and tasks that require different motor skills, or to be better prepared for unexpected tasks.

% %-------------------------------------------------------------------------
% \section{Research Plan}\label{sec:plan}
% %-------------------------------------------------------------------------

% Our research plan is divided into three activities: developing lifelong learning approaches to acquire and evolve task knowledge and motion skills (Activity 1); developing fast fast intent forecasting methods to assist robot-mediated interactions between novice teleoperators and end users (Activity 2); and implementing the proposed methods and evaluating their efficiency in interactive nursing tasks (Activity 3).

% %-------------------------------------------------------------------------
% \subsection{Activity 1 --- Lifelong learning of multi-lateral physical human-robot interaction}\label{sec:plan-motion}
% %-------------------------------------------------------------------------
% \noindent


% %Goals: %\textbf{Learning from robot-mediated multi-lateral physical human-robot interactions/collaborations, to acquire knowledge of collaborative task decomposition.} 
% \noindent
% {\bf Problem statement:} In this research activity,
% {\bf we seek to construct task knowledge and motion skills from robot-mediated collaboration demonstrated by expert teleoperators and end users}. We propose to: use motion primitives to model and generalize the temporal and spatial coordination in individual robot motions and between end users and teleoperated robots (Activity 1.1); use automata learning methods to acquire task structure and collaborative task decomposition (Activity 1.2); and combine off-line and on-line learning to achieve efficient task/user adaptation (Activity 1.3). 

% %-------------------------------------------------------------------------
% \subsubsection{Activity 1.1 --- Learning low-level coordinated and interactive motion primitives}\label{sec:plan-motion-low}
% %-------------------------------------------------------------------------

% \paragraph*{Parameterized models for low-level motion skills:} Motor primitives are the building blocks of complex and goal-directed motor skills~\cite{flash2005motor}. Inspired by human motor primitives, dynamic movement primitives (DMP) were proposed for learning and producing episodic and rhythmic motions of spatial and temporal coordination~\cite{ijspeert2013dynamical}. More recently, probabilistic modeling of motion primitives has been proposed to better address demonstration regularity and variability for imitation learning~\cite{calinon2007learning,calinon2010learning}, to account for sensing uncertainty in the feedback loop control for motion reproduction~\cite{meier2016probabilistic}, and to model the temporal and spatial dependency of human-robot interaction ~\cite{maeda2017phase}. Beyond the field of imitation learning, constructing parameterized action/motion primitives has also been investigated by the robotic control \cite{fu2013bottom} and reinforcement learning (RL) communities \cite{masson2016reinforcement}. Different from motion primitives derived using Gaussian basis regression, a parameterized action is a discrete action parameterized by a real-valued vector. For example, a navigation controller is an action that is parameterized by a goal position. For a given parameter, a low-level optimal control/motion plan can be generated. Recent work proposes a RL algorithm to simultaneously learn both the optimal parameter and the low-level policy in Markov decision processes \cite{masson2016reinforcement}. However, such an episodic approach does not address spatial and temporal dependency between a set of parameterized actions. 

% % PM: "DMP" should be "dynamic" motion primitives, not "dynamical" motion primitives (according to original paper). I've changed this throughout.
% \paragraph*{Proposed approach for learning motion primitives:} Given the demonstrated robot-mediated nursing tasks, we will perform automatic data segmentation to identify and separate motion primitives~\cite{fod2002automated, barbivc2004segmenting, meier2012movement}. We will then use the dynamic movement primitive model~\cite{ijspeert2013dynamical} to fit each motion segment, and classify motion segments using the weighting coefficients as features. For similar motions, we combinie the Gaussian Mixture Model (GMM) with Gaussian Mixture Regression (GMR) and Hidden Markov Models (HMM) to model the regularity and variability for clustered demonstration segments~\cite{calinon2010learning}. 

% Given the extracted motion regularity and variability, we propose to use different probabilistic models for learning and reproducing \textbf{two kinds of motion primitives} necessary for human-robot interactive/collaborative tasks: (1) \textit{independent motion primitives}, which account for the coordination among many degrees of freedom (DOFs) that realize the elementary functions of a robot component, such as reaching, grasping, and navigating; (2) \textit{interactive  motion primitives} account for the coordination between robot components (e.g., bimanual coordination for operating medical devices) and between human and robot (e.g., the coordination of human and robot hands in object handing-overs). Different from the pre-defined synchronization of independent motion primitives, the interactive motion primitives result in online synchronization, which requires the robot hand to move based on the observed motions of its human partner's hand.

% % PM: Changed this paragraph to future tense.
% To learn the \textbf{independent motion primitives}, we will first align multiple demonstrations in a task-specific reference frame with minimal demonstration variance, and then use dynamic movement primitives~\cite{ijspeert2013dynamical} to capture the average trajectories of the DOFs in temporal and spatial coordination. Given the motion variability found by GMM/GMR, we associate a multi-dimensional Gaussian to each basis of the dynamic movement primitive model to address the covariance among the coordinated DOFs. Our model will also consider the estimation uncertainty of robot and environment states following the example of probabilistic movement primitives ~\cite{meier2016probabilistic}. To learn the \textbf{interactive motion primitives}, we propose to extend the framework of Probabilistic Movement Primitives (Pro-MP)~\cite{maeda2017phase}, from one-to-one human-robot motion phase matching to the scenario that requires both the human nurse and nursing robot use both hands to maneuver a patient transfer bed in a cluttered patient room. In this scenario, we leverage an appropriate reference frame learning method~\cite{cederborg2010incremental,dong2012learning} that can account for the relative pose/distance between human/robot and the shared object they manipulate, and with respect to static and moving landmarks in the environment, to improve the performance of motion generalization.

% % our proposed method for learning the interactive motion primitives models the high-dimensional motion correlation among all the human/robot arms, as well as the their correlation with static and moving landmarks in the environment. To improve the generalization capability, our method will and . 




% % and (3) \textit{sensory motion primitives} address the coordination of robot motion for task execution and active sensing. Note that the nursing robot is equiped with a static camera on its chest and moving cameras attached to its wrists. It can also access external cameras in a patient room. To leverage its visual advantanges, the robot can (a) bring the manipulated object into its preferred camera view, (b) move the hand with wrist camera to observe the manipulating hand from a preferred perspective, and (c) move the body and/or hand to provide the external camera with a better view. 


% % \begin{itemize}

% % \item The \textit{independent motion primitives} model the coordination among the degrees of freedom of a robot, such as the arm joint coordination when a robot reaches to grasp an object.    Given multiple observation of teleoperated reaching motions,

% % \item The \textit{interactive motion primitives} represents the coordination between a human and robot. We extended the concept of phase matching using probablisitic movement primitives~\cite{maeda2017phase} from the context of one-to-one human-robot object handover, to multi-lateral human-robot collaboration/interaction with shared intent. Consider a nursing task of human nurse and nursing robot collaboratively moving a clumsy patient transfer bed: both the human nurse and nursing robot need to use their two hands to manuever patient transfer bed in a cluttered patient room. As a result, 

% % \item The \textit{sensory motion primitives} is proposed for coordinating the robot motions for active sensing and for performing tasks. Our previous system evaluation has demonstrated that an expert teleoperator actively adjusted the robot wrist camera on one hand to see the manipulation task being performed by the other hand. An expert teleoperator also adjust the nursing  robot's arm/body posture so that task can be better seen by an external camera. As a result, we propose to learn the active sensing motion primitives (e.g., the motion of adjusting the observing wrist camera on one arm) with the tasking performing motion primitives (e.g, the motion of reaching to grasp an boject using the other arm). We will also use the probablistic models of coordinated motion primitives to learn and reproduce the coorelation the robot's task-performing end-effector trajectories     with the trajectories observed by different cameras. 

% % , to facilitates a novice user's need when control the robot under shared autonomy. 

% % From the demostrated robot mediated interaction, we aim to  To be specific, 

% % \jie{In the last sensory motion primitive, you may want to include the learning of sensory motion primitives with vision-based planning/control. The generalization of primitives on different tasks may cause the blocking of views. It is better to plan this goal of sensory motion primitive based on what the users need to see/observe.}
% % \end{itemize}

% % is to learn from observation both temporally dependent and coordinated task sequences/collaborative task decomposition?


% % \paragraph*{Robot motion coordination in physical human-robot interactions}

% % Three types of coordination: joint coordination in low-level tasks (DMP), coordination of manipulation and navigation, coordination between human and robots in collaborative/interactive tasks (ProMP). 

% % \paragraph*{Coordinated Interactive Motion Primitives}

% % \zhi{(1) Joint coordination --- (probabilistic) DMP; (2) Interactive motion primitives --- Extended ProMP; (3) Coordination of active motion with active sensing --- coordinate the arm with wrist camera with the hand performing manipulation tasks; (4) Coordinate redundant DOFs with teleoperated DOFs --- redundancy resolution}  


% %-------------------------------------------------------------------------
% \subsubsection{Activity 1.2 --- Learning high-level task structures}\label{sec:plan-motion-high}
% %-------------------------------------------------------------------------

% \paragraph*{Abstract representations of task structure:} The nursing tasks we consider consist of many inter-dependent procedures and complex state-action relationships. Thus, it is useful to learn abstract task structures from the demonstrations of human experts to facilitate task reasoning, decomposition, and reproduction. Previous research in robot learning has used high-level task structure representations such as Finite-State Automaton (FSA)~\cite{niekum2013semantically} and skill trees~\cite{konidaris2012robot}. A high-level task plan can be built upon low-level motion primitives~\cite{konidaris2012robot} and symbolic representations of states and actions~\cite{konidaris2018skills}. 

%  \paragraph*{Learning abstract task structure:} We propose to use automata learning to acquire the abstract task structure. At a high-level, a robot-mediated task structure is a Finite-State Automaton built upon the symbolic representations for the actions, states and the action induced transitions. In addition, uncertainty in action can be captured with probabilistic automata. 
% % \todo{Jane: it will be nice if you can talk a little bit about probabilistic  probabilistic automata, since the deterministic FSA has been used in~\cite{niekum2013semantically}} 
% Both the action and state symbols are associated to their owner, which can be either the teleoperated robot or the end user, and the objects being manipulated. The action symbol is an abstract representation of a collection of compatible motion primitives, while a state symbol is a set of probabilistic distributions learned directly from robot sensorimotor data. To relate an action to object states, we observe from the demonstrations the probabilistic distributions that describe the effects of the action. Extended from the single-agent framework in~\cite{konidaris2018skills}, our high-level task representation considers the effects that can only be achieved by joint human-robot action, and the effects that can be achieved by either the human or the robot. It generalizes to probabilistic transitions to capture several possible next steps given the current action and state of human (robot), and the uncertainty in dynamic task assignment. For example, a nurse may takeover the task step of the robot under certain circumstances. The abstract representation we propose is more inclusive and flexible since it encompasses the individual human/robot tasks and robot-mediated collaboration scenarios. 



% % We also observe if there exist correlation between the actions of end user and teleoperated robot to determine the coordinated motions should be modeled using interactive or independent motion primitives. 
% Here we provide more details on automata learning. Generally speaking, abstraction-based learning seeks to infer the abstract language $L$ that describes  a set of task sequences from collaborative human experts, referred to as the \emph{language of the teleoperation system}. During interactions, the autonomous system only observes finitely many instances from this language and the goal is to generalize from such observed high-level task sequences to an automaton or grammar representation of the language that may include infinitely many possible task sequences. For this task, grammatical inference (GI) \cite{de2010grammatical}, also known as automata learning, is appropriate as an abstraction-based learning mechanism. GI is a class of algorithms that generalize from a finite number of sentences to provide a grammar that describes the language, which may contain infinitely many words. As an example, consider the tele-operated robot accomplishes two tasks:  ``\textbf{P}ick up the cup (P) and \textbf{H}and it over to the nurse (H)'', and 
% ``\textbf{M}ove to the table (M), \textbf{P}ick up the cup and pla\textbf{C}e it on the shelf, and pick up the bag and hand it over to the nurse.'' These two task sequences generate a \emph{prefix}-tree, similar to a skill tree \cite{konidaris2018skills} in Fig.~\ref{fig:gi} (a) that accepts exactly two words, i.e., task sequences. Using the state merging operation \cite{de2010grammatical} in the GI algorithm, the system is able to generalize to the automaton in Fig.~\ref{fig:gi} (b), and understand it can pick and place objects infinitely often, indicated by the loop. Lastly, with another state merging, the robot learns it is able to relocate to different positions with action ``M'' (move), and can pick and place an object or hand an object over to the nurse. Using the inference algorithm, the automaton learned by GI generalizes from a skill tree to an automaton or grammar that represents a potentially infinite set of possible task decompositions and task sequences.


% \begin{figure}[t!]
%     \centering
%     \begin{subfigure}[b]{0.4\textwidth}
%         \centering
% \includegraphics[width=\textwidth]{fig/original.pdf}        \caption{}
%     \end{subfigure}%
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
% \includegraphics[width=\textwidth]{fig/merge1.pdf}         \caption{}
%     \end{subfigure}%
%     \begin{subfigure}[b]{0.28\textwidth}
%         \centering
% \includegraphics[width=\textwidth]{fig/merge2.pdf}     
% \caption{}\end{subfigure}
%     \caption{An illustration of the grammatical inference algorithm using state merging. (a) is the original prefix tree/skill tree. (b) generalizes (a) by merging state $3,5$ and $4,6$. (c) further generalizes (b) by merging $0,3,5$, $1,4,6$, and $2,7$. The state merging or splitting is guided by a GI algorithm selected for the target language on positive data only, or with both positive and negative data.}
%     \label{fig:gi}
% \end{figure}


% In this specific application context, we will consider several GI algorithms, including learning from positive demonstration under the assumption that actions have pairwise dependence \cite{heinz2013learning}, and the $L^\ast$ algorithm, which learns from both positive and negative demonstrations \cite{angluin1987learning}, given that negative demonstrations may be obtained from the nurses’ feedback of unreasonable behavior. We propose to learn, for three interacting agents, their individual alphabets and languages. We seperate the concern of  action synchronization, conflict, and sequencing from learning and propose to capture such constraints for collaboration and coordination through product operations between languages/automata, including but not limited to, synchronization composition and sequential composition \cite{alur2015principles,hopcroft2001introduction}. 
% For instance, synchronization will enable us to capture the dependency between the action of aiming the camera at the hand of the human nurse with the reaching motion of the manipulator.  Inference algorithms for probabilistic finite-state automata \cite{clark2004pac} \cite[Chap 12]{de2010grammatical} will further enable us to capture stochastic transitions resulting from uncertain outcomes of actions and uncertainty in task step coordination. 

% \paragraph*{Learning shared object affordances:}
% %When human and robot cooperatively manipulate a shared object, we will use the knowledge of shared affordance  to determine if human and robot actions are compatible. 
% Object affordances define the operations that an object allows a human/robot operator to perform~\cite{gibson2014ecological}. By learning these affordances, we aim to establish the relations between the consistent changes of the object states and the robot’s action sets. As high-level task knowledge, object affordances not only enable objects to be categorized by their functions, but also correlate a robot's general low-level motor skills with specific task contexts~\cite{fitzpatrick2003learning, veloso2005learning, thomaz2009learning,lopes2007affordance,montesano2008learning, lopes2010abstraction, kjellstrom2011visual,aksoy2011learning}. Research efforts thus far have investigated the affordance of an object manipulated by a single operator. Yet, it is still unclear in cooperative manipulation tasks, given operations the human and robot partners can apply to the shared object, what task sharing will be appropriate. Consider a human nurse and a nursing robot collaboratively removing a dirty blanket from a patient.  Given the position where the human nurse has grasped the blanket, the robot needs to grasp the other corners so that the blanket will not be dragged away but will be lifted and removed instead. Although the blanket affords grasping at any point, as a shared object in cooperative manipulation, the robot operation it can afford is determined by the applied human operation. 

% To represent the knowledge of shared object affordances, we will create a hierarchical object affordance library that categorizes objects by their functions and associates the constraints on the object states during the cooperative manipulation (e.g., balance a food tray while lifting) 
% to objects in the same sub-category. We will then use these constraints to evaluate if a selected joint action of the human and the robot is compatible. For instance, when a human nurse and tele-nursing robot collaboratively move a patient transfer bed, or a human nurse hands over a medical tool to a robot, the shared object affordance will inform the robot where to grasp given the observed human partner's grasping positions.
% % \todo{JF: Is there other form of shared object? two mentioned two examples are similar.}

% %-------------------------------------------------------------------------
% \subsubsection{Activity 1.3 --- Fast adaptation of motor skills to personalized user needs}\label{sec:plan-motion-update}
% %-------------------------------------------------------------------------
% Through low-level and high-level learning, we shall construct the motion intelligence of the mediating robot consisting of general task knowledge and general motor skills. Here we further propose to %framework that 
% employ the robot's motion intelligence for fast adaptation to the personalized needs of teleoperators and end users. Our key idea is built on two-level motion knowledge: A robot's motion intelligence should include \textbf{General Motion Templates}---a motion knowledge hierarchy from low-level motion templates to high-level motion plans, and \textbf{Special Skills}---highly customized motor skills that address a user's particular needs in an activity. The two aspects of motion knowledge differ in their generality and thus require different strategies for acquisition, maintenance, and refinement. To this end, we propose a lifelong learning and adaptation method to integrate knowledge-based motion control and online data-driven motion adaptation seamlessly. This method will allow the system to continuously improve its motion intelligence without the burden of maintaining a large database and increased local computational efforts. 


% To illustrate our proposed method, consider a nursing robot learning collaborative manipulation for a new object from robot-mediated interaction. Before being deployed to the new task, the robot has been trained off-line and acquired general motor skills for manipulation, such as coordinated reaching and grasping. Using a large and general demonstration dataset, the offline training can derive robust computational models for the robot end-effector's reaching motion primitives and frequently grasping hand pre-shapes (i.e., the general motion templates). However, the offline training is not informed of the personalized and/or task-specific needs. For example, in collaborative nursing tasks, the robot should render arm postures that are natural and accommodating to its collaborating human nurse. At the same time, by observing the teleoperator in a new task, the robot will be notice that compared to the reaching motion primitives it has, the movement primitives extracted from the new demonstrations are significantly different at the degree of freedom that measures the arm posture (i.e., the rotation of the elbow with respect to the should-wrist axis, namely the swivel angle~\cite{Tolani_Badler:96}), but are quite similar at the degrees of freedom for end-effector trajectories. Given the similarity and difference, the robot will propose to the teleoperator to perform incremental learning to update motion primitive models for the end-effector trajectories~\cite{havoutis2016online}. To be specific, we will use DP-means algorithm~\cite{kulis2011revisiting}---an online GMM building algorithm---to incrementally update the GMM/GMR model generalized over multiple off-line demonstrations, based on the the squared Euclidean distance from the new data to the GMM cluster centers. Base on the same idea, decremental learning can be performed to remove obsolete demonstrations. In addition, the motion template for the swivel angle trajectory must be overwritten by special skills in specific tasks (after confirming with human expert), if the new demonstration and general motion template have significantly different DMP models. To learn an expert teleoperator's skill of adjusting a pre-defined hand pre-shape to new objects, we will combine model-based and model-less  approaches to classify the observed hand manipulation, and compare their performance in generalization. 
% % PM: Maybe I'm missing something here but is an SVM really model-based? Moreso than a CNN? IIRC SVMs and CNNs are equally data-driven and neither uses any kind of strong prior or other model for initialization.

% % \todo{JF: need some detail about incremental learning, the reference \cite{hewitt2017dmp} does not seem to address this.}

% %  From a system abstraction viewpoint, the set of high-level tasks that can be achieved jointly by the teleoperation system, the human nurse, and the patient, can be understood as a \emph{language} $L$ over the \emph{alphabet} $\Sigma$. Each symbol in the alphabet corresponds to a low-level closed-loop controller with certain parameters. For example, approaching a desired goal position is achieved with a navigation control parameterized by a goal position and velocity, while picking an object is parameterized by the object affordance, force control, and object position. With the low-level parameterizable control actions, i.e., alphabet, a high-level feasible task becomes a \emph{sentence composed of symbols with appropriate parameterization}. For example, the concatenation of go to the table ($a(p_1)$) and pick up the cup on the table $b(p_2)$ is only possible if the goal position $p_1$ makes the cup within the reachable workspace of the manipulator. This is similar to pre- and post- conditions of actions in situation calculus \cite{}, except that we now use parameterized actions to enable generalization and flexible reusing of the low-level controller. Building with parameterized actions, we construct symbolic models of a dynamic system with methods \cite{fu2013bottom}. For time-invariant systems, the symbolic model can be represented as an automaton, which is a representation of a language.

% %Our goal is to achieve high-level task learning and reasoning by integrating this abstraction-based control synthesis for hybrid systems with abstraction-based learning from demonstrations.





% % In addition, we also consider the approach of constructing skill tree~\cite{konidaris2012robot} and extend this concept to \textbf{shared skill tree} to encompass the robot-mediated collaboration scenario.  In the framework of shared skill tree, we will integrate the idea of high-level symbolic representations to model the tree component that involves complex collaboration~\cite{konidaris2018skills}. This method abstracts actions and states as symbols and therefore formulates a complex planning task to set and propositional operations. To adapt to the robot-mediated collaboration scenario, we revise the framework in~\cite{konidaris2018skills} by associating the symbolic representation of actions and states to their human/robot owner, so that the high-level task representation can be more inclusive and flexible. 







% % and space of possible collaborative task decompositions from the interactions between human end users and a teleoperated nursing robot. To illustrate collaborative task decomposition, consider two typical scenarios for collaborative tasks: 

% % \begin{itemize}
% % \item {\it A task without a shared object}: For a task that consists of multiple sequential/parallel subtasks, human and robot are responsible for different subtasks but do not interfere with each other. Although they do not cooperatively manipulate any shared object, they affect each other's action and performance because (1) the step completed by the robot must precede the actions of its human partner and (2) the effectiveness of the robot at its current subtask %step (measured in terms of accumulated reward) 
% % will influences its partner's efficiency in the next subtask. %cause its partner to improve, maintain, or decrease performance in the next step. 
% % In this case, the collaborative task decomposition will define which subtasks are assigned to the human and robot by matching their physical capabilities to the task affordance. Particularly, for the steps that the robot is responsible for, it defines the acceptable range of final states. % and/or acceptable performance measured by accumulative reward in the collaboration.  

% % \item {\it A task with a shared object}: For some subtasks, the human and robot cooperatively manipulate a shared object. In this case, the collaborative task decomposition specifies which part of the object is possible for the human/robot to manipulate given the object affordance, in addition to possible subtask role assignments and acceptable final states. % and task performance. 
% % For instance, for the task that human nurse and nursing robot cooperatively manipulate a patient transfer bed, where the robot can grasp and push the bed depends on where the human nurse grasps and pushes, and vise versa. As the shared object involved, the temporal and spatial motion coordination between human and robot becomes more critical. It is also important to extend the concept of individual object affordance to shared object affordance and define their affordable operation in abstract representation. 

% % \end{itemize}

% % % \paragraph*{Abstraction-based control and learning}
% % \paragraph*{Learning abstract representations of collaborative task decompositions}
% % % \zhi{this can be used to learn high-level collaborative task decomposition}
% % % To learn from the robot-mediated interaction, explicit learning and programming each task sequence is not only time consuming but also inflexible given any changing environment, e.g. different patient needs, changed operating rooms, cooperated nurses.
% % We consider multiple approaches for learning the abstract representation of collaborative task decompositions. Our first approach is 
 
% % \jie{It is a bit lengthy and tedius, I put it here for now but we will see if it is truly needed.}
 
% %  We seperate the concern of low-level learning and describe later a statistical model will be used to capture low-level parameterization—similar to goal-directed intent— due to the inherent nondeterministic decision making or behavior uncertainty of humans. Instead of positioning at a specific pose, statistical methods will learn a distribution of possible poses for manipulation tasks using intention-aware planning \cite{bandyopadhyay2013intention}, where the goal of manipulation is treated as a hidden parameter and can be inferred from the demonstration using Bayesian reasoning.



% % The high-level learner abstracts away low-level parameter but focuses only on the behavior coordination and task decomposition level. Next, we propose methods for low-level learning of parameterized actions. 
% %  to (1) use high-level symbolic representation for task modeling at high-level, 


% % present CST, an online algorithm for constructing
% % skill trees from demonstration trajectories. CST segments a demonstration trajectory
% % into a chain of component skills, where each skill has a goal and is assigned a suitable abstraction
% % from an abstraction library. ese properties permit skills to be improved efficiently using a policy
% % learning algorithm. Chains from multiple demonstration trajectories are merged into a skill tree.

% % \zhi{I will also add GEorge Konidaris' method of Constructing Skill Trees by Demonstration, and extend his framework to shared skill tree that consist of the skills of on-site human nurse and the skills of robots --- this could be an alternative method for learning collaborative task decomposition; Also refer to his work on Learning Symbolic Representations for Abstract High-Level Planning }




% % \paragraph*{Shared autonomy through Task-specific generalization:}
% % Intelligent task affordance learning should not only reproduce expert demonstrations for the set of training tasks but also generalize to achieve good performance for tasks that have never been encountered before. At low-level motion described by DMP, action generalization methods has been developped in \cite{ude2010task}. The method generalizes from a set of DMP parameters, each of which is for  a specific configuration, to a function $f: q \mapsto (w, \tau, g)$   that maps from the continuous configuration space to the continuous DMP parameter space (linear parameter, phase, and goal). We propose a two-tier approach to combine task-specific DMP generalization with goal inference and task affordance learning to develop guided assistance for novice teleoperator. The approach decouples motion guidance from task guidance: Task guidance directly relates to task affordance learning and aims to  guide the novice user step by step in completing a specific dynamic task with sequential composition of local subtasks. Motion guidance aims to generate guiding force via haptic devices so that the user can be assisted to achieve motions that requires dexterous manipulations, such as reaching in cluttered environment, reaching and grasp with autonomous object affordance reasoning for the best grasp.

% % During the cooperative manipulation, the robot keeps monitoring the trajectories of the object's manipulation-related states,  


% % , and (2) the set of motion primitives that model the  trajectories of the object's manipulation-related states during their cooperation

% % Under shared-autonomous control, if the supervisory teleoperator notice 





% % the model the temporal and spatial correlation between human/robot hands with respect to each other, and with respect to the moving center of the blanket.  

% % \zhi{(1) Learning functional object affordance; (2) Learning shared object affordance from collaborative manipulation} 



% % \textit{\textit{\jie{goal-directed learning shall be coupled with inverse reinforcement learning to capture not only the goal, but how the human operator achieves the goal.}

% % \jie{Possible subtasks based on discussion: Jane suggested skill tree learning and low-level DMP learning. Propose to use shared intent inference} 

% % \zhi{Learning the coordination of mobile base with manipulation --- as far as the reward function concerned, here are several coordination policy hypothesis: manipulability, reachability, vision advantage (perception convenience and comfort); minimal intervention of macro-structure (also relevent to energy consumption); environment constraints (static obstacle, prediction of moving obstacle and agents)}
% % }}

% % \paragraph*{Share intent inference}

% % * What is the problem?

% % * Formulation (mathematical)

% % * proposed approaches (at least two): 


% % \paragraph*{Problem Formulation}
% % * Task 1 - Learning physical interaction with nurse and with patient 

% % 	* Demo 1 - Human nurse hand-over object to patient

% % 		* Learning object/task affordance
% % 		* Learning the phase matching in human-human interaction
% % 		* Learning how the human nurse adapt to the patients with limited motor capability 
% % 	* Demo 2 - Expert teleoperator and human nurse perform robot-mediated collaborative task

% % 		* Human nurse leads the task
% % 		* Learning from teleoperator's responsive motion to the end user 
% % 		* Robot controlled by the teleoperator can learn from the teleoperator the feasible and appropriate operation given the robot's limited physical capability and teleoperation transparency
% % 		* Teleoperator can communicate with human nurse on-site tof negotiate via bidirectional telepresence

% % 			* Teleoperator who can not precisely perceive the spatial and temporal information of the task from inconvenient and/or insufficient camera view
% % 			* The teleoperator can receive feedback in terms of verbal critique and gestures from the end user
% % 			* The bilateral communication between teleoperator and end user can compensate the lack of information and correct the motion errors due to mis-perception


 



% % Similar to the fast adaption of low-level motor skill, our framework can also update the high-level collaborative task affordance, including the 


% % Building upon these results, it is possible to develop a self-evolving motion intelligence, which can \textbf{acquire} user-specific and customized motor skills based hierarchical motion knowledge and generalized skill sets, can \textbf{reflect} upon its motion knowledge hierarchy to refine its motor skills and improve its learning efficiency.   

% % PM: Didn't review the commented stuff above

% %   which can customize the general motor skills of the 

% % incremental and decremental learning

% %-------------------------------------------------------------------------
% \subsection{Activity 2 --- Fast intention forecasting and blended control for robotic mediation}\label{sec:plan-intent}
% %-------------------------------------------------------------------------
% \noindent
% {\bf Problem statement:} For interactions between humans and robots in nursing tasks, shared intent is the key to successful collaboration.
% Thus, the robot must be able to infer the intentions of nurses, teleoperators, and patients, at both the level of motion primitives and at the level of task decompositions and assignments to appropriately mediate the joint task through shared autonomy. In this activity, we propose to leverage the knowledge and models we constructed in Activity 1 to provide: (1) shared intent forecasting of high-level tasks, sub-tasks, and role assignments within a cooperative game formulation; and (2) intent forecasting of subtasks from motion primitives through robot mediated demonstrations. We then use these intention forecasts to enable: (1) autonomous control of the robot's navigation based on teleoperated control of manipulation; and (2) assistive teleoperated manipulation based on intent forecasting and task knowledge.
% %Here, we focus on novice teleoperators and relax the strong coordination and near optimality assumptions employed to learn from expert teleoperators in Activity 1.

% \noindent
% {\bf Probabilistic approach:}
% Following existing probabilistic reasoning methods for intent \cite{wang2013probabilistic},
% we estimate intended tasks, denoted as ``language'' L, from a partial sequence of subtasks, denoted as ``states'' $s_{1:t}$ and the (partial) motion primitive(s), denoted as controls $u_{t+1}$, currently being executed by each participant using Bayes theorem and hierarchical/Markovian properties:
% \begin{align}
% & P(L|u_{t+1}, s_{1:t}, c) \propto \sum_{s_{t+1:T}} P(u_{t+1}|s_{t+1}, c) P(s_{1:T}|L, c) P(L|c); \label{eq:forecast}\\
% & P(s_{t+1}|u_{t+1}, s_{1:t}, c) =
% \sum_{L}\sum_{s_{t+2:T}} P(s_{t+1:T}|s_t,L,c) P(L|u_{t+1}, s_{1:t},c),
% \end{align}
% where $c$ is a generic form of additional contextual information.
% Key quantities for estimating these low-level sub-tasks and high-level tasks are 
% the likelihoods for the high-level task decomposition and assignment, $P(s_{1:T}|L,c)$ and
% (partial) motion primitives under different subtasks, $P(u|s,c)$. We discuss models for each of these in the following sections.

% %-------------------------------------------------------------------------
% \subsubsection{Activity 2.1 --- Inverse Reinforcement Learning for Task Decomposition and Role Assignment}\label{sec:plan-intent-Stackelberg}
% %-------------------------------------------------------------------------

% % \todo{Jane: Given the high-level task structure learned in Activity 1.2, activity 2.1 uses Stackelberg Game Formulation to infer the collaborative task assignments between expert teleoperator and an on-site human nurse. The demonstration task can be human and robot collaboratively organize and clean the patient room. For such a task, activity 2.1 will provide a reward function to optimize the task allocation, particularly for the subtask that both robot and human nurse can do -- such as who is going to pick which object, and who is going to fetch a far-away object --- to optimize task efficiency and accommodate human nurse's preference. }

% \noindent
% {\bf Problem Formulation:} Using the space of task decompositions for a given task defined by language $L$ in Activity 1.2, we seek to learn the preferred decomposition and the assignment of the human partner and robot to the sub-tasks of the decomposition. To support probabilistic reasoning in Eq. \eqref{eq:forecast}, we construct a probabilistic model of sub-tasks for the given task, $P(s_{1:T}|L,c)$, where $s_t$ represents both the sub-task and role assignment of each participant (e.g., robot, nurse, or patient) at subtask step $t$. Though we already have a model of this kind from the probabilistic FSA, it cannot easily incorporate the available contextual information, $c$, relevant to the task.

% \noindent
% {\bf Sequential Game Formulation:} 
% The nature of collaboration leads us to consider the problem of coordinating behavior using game-theoretic formulations: In this game, two agents collaboratively perform some mutually desired task. 
% Task assignments can be formulated as a Markov game $M=(S, A_1\times
% A_2, P, R, F)$ where $S$ is a set of states, each including the joint
% state of all agents and an auxiliary task-related
% state. The set $F\subseteq S$ is a set of \emph{task completion}
% states: the task is completed only if a state in $F$ is reached. For
% example, in collaborative pick-and-place of multiple utensils,
% the task completion state is reached only when all objects are
% collected. The reward function $R$ specifies the \emph{unknown
% end user's preference} of task completion, which determines a
% policy for task assignment in collaboration. 
% The observed demonstrations, including the demonstrations of the robots and those of the nurse, should result as a solution of the cooperative game in which both agents act jointly to maximize a performance criteria expressed as a reward function $R: S_{t,1} \times S_{t,2} \times S_{t+1,1}\times S_{t+1,2} \times C \rightarrow \mathbb{R}$ where $S_t$ and $S_{t+1}$ are consecutive sub-tasks is the world state of two collaborators (e.g., the robot and the nurse) and can sometimes be a shared cooperative sub-task, as shown, or factored as $R_1 + R_2$ with $R_i: S_{t,i} \times S_{t+1,i} \times C \rightarrow \mathbb{R}$, and $C$ is additional contextual information.

% \begin{wrapfigure}{r}{0.35\textwidth}
% \includegraphics[width=0.35\textwidth]{fig//softstar.png}
% \caption{Simple illustration of the heuristic-guided search approach to approximate probability estimation between two endpoints (white states).  The set of states in the solid black oval are active. The additional ones inside the dotted oval are on the fringe.}
% \label{fig:softstar}
% \end{wrapfigure}
% \noindent 
% {\bf Centralized Maximum Entropy Inverse Reinforcement Learning for Cooperative Tasks:} We propose to extend inverse reinforcement learning methods \cite{ziebart2008maximum} to learn utilities for the selection of a specific task decomposition and assignment of its subtasks in the sequential turn-taking order of the Stackelberg game representation~\cite{basar1999dynamic}. These learned utilities are estimated to rationalize coordinated behaviors that have been demonstrated.  
% % PM: What is a "Stackelberg" game? Is that the formulation you described above? In current text, this hasn't been mentioned up to this point.
% However, our approach can be viewed as ``centralized'' in the sense that each player's conditional probability distribution is not separately modeled.
% Following the Markov game representation, we define utilities in terms of the current state and joint actions from all agents. These will often be independent sums of each agent's individual utility when employing independent motion primitives or sensory motion primitives. 
% We consider maximum entropy inverse reinforcement learning \cite{ziebart2008maximum} generalized to use minimum relative entropy with respect to a reference policy $P_0$.
% Under this formulation, a particular task decomposition and assignment $s_{1:T}$  from language $L$ with reward $R(s_t,s_{t+1},c)$ is predicted with probability $\prod_{t=1}^{T-1} P_0(s_{t+1}|s_t) e^{\sum_{t=1}^T R(s_t,s_{t+1},c)}/Z$, where $Z$ is a normalization term over the entire language and possible assignments: $Z = \sum_{s'_{1:T}\in L} \prod_{t=1}^{T-1} P_0(s'_{t+1}|s'_t) \sum_{t=1}^T e^{\sum_{t=1}^{T-1} R(s'_t,s'_{t+1},c)}$. The rewards $R$ are estimated by maximizing the likelihood of demonstrated collaboration sequences.

% Unfortunately, the normalization term $Z$ can be difficult to compute exactly since the language $L$ is often not finite.
% We propose to extend heuristic-guided methods for single-agent maximum entropy inverse reinforcement learning \cite{monfort2015softstar} to this cooperative multi-agent setting.
% The key idea is to uncover a subset of the state-space for language $L$ that captures most of the probability mass for computing $Z$.
% This is illustrated in Figure \ref{fig:softstar}. Upper bounds (heuristics) for the contribution of sequences through states in the fringe are used to incrementally expand the active set of states (black oval) and bound the overall approximation.
% Cooperative tasks, though often incorporating large effective state  spaces (based on language $L$), have some distinct advantages for this approximation scheme.
% First, many of the tasks are broken down into individually-executed sub-tasks; the ``probability mass'' of these can be computed independently and then integrated into the overall estimate.
% Second, heuristics can be naturally obtained by making relaxations to some of the participants constraints and by assuming ideal contextual variables; for example, assuming that the collaborative partner is able to optimally assist with every sub-task ignores constraints for the partner's capabilities, but still provides a useful heuristic.
% Third, so long as the reference policy $P_0 < 1$ and some regularization of the estimated rewards is imposed, the entropy of the resulting distribution must be finite, which avoids the tendency of standard maximum entropy IRL towards a uniform distribution over trajectories---typically with infinite expected length \cite{ziebart2010modeling}.

% % PM: Not reviewing commented text

% % \subsubsection{Activity 2.1: Forecasting the intent of motion primitives from robot-mediated demonstrations} 
% % %In this subtask, we examine the question: Is it possible to infer tele-operator's intent from the robot mediated demonstrations? The purpose of inferring teleoperator's intent has two-fold: On one hand,
% % An understanding of motion primitive intent can enable the robot to use blended control \cite{downey2016blending} to ease the effort required from a novice teleoperator to achieve the control objective. For example, in a task of rearranging utensils on a table, with the ideal blending control, the teleoperator maintains control of high-level movements, such as moving the hand towards the cup (\emph{intent}) and when to grasp (\emph{intent}), while the autonomous robot handles the details of this task and performs intuitive, human-like motion using DMPs. This is only possible if the robot knows which object the user intends to pick up. 

% % %On the other hand, for the purpose of training novice teleoperators, if we obtain imperfect tele-operation demonstrations, how to infer the underlying intent and aid the novice user? \jie{I think it is important. However, the current method cannot do this.}

% % To answer this question, we formulate the problem in IRL setting: the tele-operator has an unknown reward function for which his behavior is assumed to be near optimal. The reward function captures goal-related reward and risk-averse behaviors, as well as other performance criteria such as efforts involved in operating the input devices. 
% % The challenge is that in this IRL problem, we obtain two demonstrations simultaneously: One demonstration $d_h$ is given by the tele-operator using the input device and demonstration $d_r$ is given by the tele-nursing robot. The mapping from $d_h$ to $d_r$ is in general a nonlinear function internalized by the expert. If we simply seperate the demonstrations into $D_h$ and $D_r$, and apply IRL to solve two separate reward functions, we remove the coupling between the reward functions and the unknown mapping. Another approach is to consider a joint reward function $R$ defined over the product state space between the workspace of tele-operator and the workspace of the robot and use IRL algorithm to learn $R$. A potential problem with this approach is that it does not distinguish the performance criteria for tele-operation and the user's preference for tele-operation. To address this, we propose an approach inspired by planning in shared autonomous MDPs \cite{fu2016synthesis}: Consider there are two reward functions $R_h$ and $R_r$, defined over the product state space between the workspace of tele-operator and the workspace of the robot. The behavior of the tele-operated system can be viewed as if the system performs a Pareto-optimal policy that aims to balance maximizing $R_h$ (goal-reaching, smoothness, time-efficiency in tele-operation) and $R_r$ (goal-reaching, safety, efforts in robot maneuvers): A policy is Pareto optimal if there is no other policy that can make it better for one objective than this policy without performance degradation for another
% % objective. Thus, the IRL problem needs to solve (1) reward functions $R_h$ and $R_r$, and (2) a pre-defined weight vector $w_h, w_r$ that pinpoint on solution from the Pareto optimal set. Our approach is illustrated in Fig.~\ref{fig:pareto_irl}, where for a given set of weights, we employ the Chebyshev scalarization method \cite{perny2010finding} to find Pareto-optimal policies. In addition to reward inference, the inference of weights will be treated as a hyperparameter optimization problem using the primal and dual formulation of maximum likelihood estimation in IRL setting \cite{franceschi2017forward}.
% % \begin{wrapfigure}{r}{0.6\textwidth}
% %   \begin{center}
% %     \includegraphics[width=0.58\textwidth]{fig/IRL_game1}
% %   \end{center}
% %   \caption{A block diagram showing the proposed IRL algorithm for intent inference for (1) teleoperator's intent through robot mediated demonstrations (Blue) (2) shared intent between teleoperator and end users for cooperative tasks (Green). Note in (1), the parameters $\theta$ includes a tuple $\theta_h, \theta_r$. }
% %   \label{fig:pareto_irl}
% % \end{wrapfigure}
% % \jie{This figure can be edited here: \url{https://goo.gl/hkyinA}}

% % The seperation of two rewards allows us to further consider personalizable tele-operation, as human individual may have different preferences on how to use the input device, characterized by $R_h$, but share a common knowledge on how the robot shall behave, given by $R_r$. Learning $R_h$ will help us to understand how to improve end user's experience. Most importantly it is possible to learn $R_h$ from simulated experiments. \jie{Jane: I am not sure if it is correct. still think about it.}

% % \paragraph*{Intent prediction for coordinated tasks with end-users} In this subtask, we focus on the problem of infer the shared intention between human nurse and expert teleoperator. By inferring shared intention, the robot will be equipped with knowledge of how to coordinate its actions or motion plans based on the observed behavior of human nurses. This intent inference largely differs from existing IRL methods: Instead of answering ``what and how the demonstrator intents to do'', the robot aims to answer ``what shall we do together as a team'', by observing a partial demonstration from its interacting human nurse. 

% % Is it possible to infer the shared intent, represented by the team reward function $R$, from demonstrations $\mathcal{D}$? Formulated as IRL problem, one possible approach is to substitute the solution by that of cooperative Markov game in Fig.~\ref{fig:pareto_irl}. However, cooperative Markov game with independent decision makers is difficult to solve  due to the coordination problem \cite{fulda2007predicting}. \todo{replace this ref if better one can be found.}


% %The task assignments in collaborative teleoperated robot and
% %nurse team can be formulated as a Markov game $M=(S, A_1\times
% %A_2, P, R, F)$ where $S$ is a set of states, each including the joint
% %state of all agents and an auxierary task-related
% %state. The set $F\subseteq S$ is a set of \emph{task completion}
% %states: the task is completed only if a state in $F$ is reached. For
% %example, in collaborative pick-and-place of multiple utensils,
% %the task completion state is reached only when all objects are
% %collected.  The reward function $R$ specifies the \emph{unknown
% %end user's preference} of task completion, which determines a
% %policy for task assignment in collaboration. 

% \paragraph*{Inverse Reinforcement Learning in a Multi-Objective Decentralized Stackelberg Game}
% To find a decentralized solution to the cooperative task game, which can be used for decentralized planning and control rather than forecasting intent, we employ an important simplifying assumption---that the human partner takes a lead role and sub-task selection is coordinated to be consecutive. This is based on observations that 
% hierarchy and role assignment are often observed in collaborative human teams \cite{mortl2012role,colman2014explaining}. For example, a human nurse may take the leader role and guide the tele-operated robot
% %, or the teleoperator indeed,  
% % PM: Ah, here's the definition of a Stackelberg game. Needs to be moved higher somehow or the reference above needs to be cut?
% towards the goal position when they collaboratively transport a patient bed. Thus, it is meaningful to consider a class of hierarchical games, called cooperative Stackelberg games \cite{basar1999dynamic}, to capture the selection of the particular task decomposition and assignment of roles to subtasks selected by the collaborative pair.

% \begin{wrapfigure}{r}{0.55\textwidth}
% \vspace{-2ex}  \begin{center}
%  \includegraphics[trim={3.0cm 0cm 0 0},clip, width=0.5\textwidth]{fig/IRL_game}
%   \end{center}
%   % PM: I Added this vspace to keep the caption away from the text below; the figure seems to include excessive bottom padding anyways.
%   \vspace{-2.0ex}
%   \caption{A block diagram showing the proposed IRL algorithm for intent inference for end users for collaborative task assignment. $\theta$ is the parameters in the unknown reward function.}
%   \vspace{0.5ex}
%   \end{wrapfigure}

% To this end, we propose the Stackelberg game formulation for IRL where the end user performs the role of a leader and the robot performs the role of a follower: When the end user decides their policy, they will safely assume that the robot (follower) will pick a best response policy
% in collaboration to the proposed leader policy. In parallel to the hierarchy in role assignments, this game is by nature multi-objective: Maximizing the end user's reward $R$ while ensuring overall (time or energy) efficient task completion. Hence, we propose to incorporate
% Pareto-optimal solutions in multi-objective cooperative
% Stackelberg equilbrium: A policy is Pareto optimal if there is no
% other policy that can make it better for one objective than this
% policy without performance degradation for another objective. Given that we know the task completion reward, the IRL problem needs to solve (1)
% reward functions $R$ that encodes the preference of the end user,
% and (2) a pre-defined weight vector $w$ that pinpoints a solution
% from the Pareto optimal set. We plan to extend our previous work on Pareto-optimal planning in Markov decision processes for shared autonomy systems \cite{fu2016synthesis} to computational methods for cooperative Stackelberg games \cite{vorobeychik2012computing,cermak2016using} for solving the inner problem in IRL. Further, to ensure a uniform spread of Pareto-optimal solutions given a uniform spread of weight vectors, a Chebyshev scalarization scheme \cite{perny2010finding} will be introduced. The inference of weights will be posed as a hyperparameter optimization problem using the primal and dual formulation of maximum likelihood estimation. % in IRL setting. 


% %-------------------------------------------------------------------------
% \subsubsection{Activity 2.2 --- Recognizing Task Steps from Partial Motion Primitives}\label{sec:plan-intent-prediction}
% %-------------------------------------------------------------------------

% We now focus on recognizing the sub-task $s$ from a partial execution of the motion primitive $u$. To leverage this within our probabilistic formulation, in which it is a key component for forecasting low-level and high-level intentions, we estimate $P(u|s)$---the probability of the control sequence given the sub-task.

% \paragraph{Similarity-based estimates:}

% The first approach we consider is similarity-based estimates using exemplar motion trajectories. For these, the similarity between a partially executed motion trajectory and a set of motion trajectories corresponding to different subtasks are evaluated and used to define the likelihood function. The simplest of these is the Euclidean distance and a Gaussian distribution: $P(u|s) = e^{-\text{dist}(u_t,u_t^*(s)})$.
% However, other distributions, such as the Laplacian distribution can also be employed. Weightings for different components of the distance function (e.g., hand versus elbow position) can then be optimized to maximize the likelihood of correct associations.

% \paragraph{Inverse reinforcement learning:}

% Our second approach is based on the application of maximum entropy inverse reinforcement learning for linear-quadratic regulation problems \cite{ziebart2010modeling}. Under this approach, a quadratic cost function ${\bf C}_s$ is learned that penalizes the state-control vector at each point in time, yielding a distribution: $P(u|s) \propto \exp{\sum_{t=1}^T [u_t \; x_t] {\bf C}_s [u_t \; x_t]^T}$. Using our previously developed system \cite{monfort2015intent,schultz2017goal}, we train this model from goal-directed teleoperation and provide prediction of the task step (i.e., identifying the intended end effector position for grasping or placement).

% \paragraph{Recurrent convolutional neural network: }

% Our third approach leverages the fact that our motion primitive models (Activity 1.1) provide a means to generate large amounts of synthetic control trajectories for different task steps and contexts. 
% We will provide these trajectories and task context (e.g., object positions and characteristics) to a recurrent convolutional neural network \cite{veeriah2015differential} trained to probabilistically predict task step labels at each timestep using a softmax activation function. Additionally, we will augment the trajectory data with video data from trajectories executed on the robot or trajectories of the human collaboration partner to learn to recognize task steps from video as well.
% % \todo{We will compare our approach in practice with methods using Bayesian network \cite{}.}

% %\todo{Jane: following the framework of Wang, Zhikun, Katharina Mülling, Marc Peter Deisenroth, Heni Ben Amor, David Vogt, Bernhard Schölkopf, and Jan Peters. "Probabilistic movement modeling for intention inference in human–robot interaction." The International Journal of Robotics Research 32, no. 7 (2013): 841-858, propose a framework that predict interactive goals bounded by joint reward functions and joint action affordance constraints}

% %-------------------------------------------------------------------------
% \subsubsection{Activity 2.3 --- Autonomous Coordination of Navigation with Teleoperated Manipulation}\label{sec:plan-intent-SharedAutonomousCoordiantion}
% %-------------------------------------------------------------------------
% % Goal: Develop a user assistive module for controlling the coordinated motion of mobile humanoid nursing robot

% % \todo{Jane: This activity aims to use IRL to learn a reward function from an expert teleoperator for how to autonomously move the mobile based to facilitate the manipulation being performed (a typical IRL problem). This reward function may tradeoff among manipulability, clearance to obstacle, vision advantage, minimal intervention to reduce energy cost, alignment with the pushing direction. This activity leads to a user assistive module that allow a novice user focus on control of hands. The novice teleopertor can simply click a button and the base will be moved autonomously.}. 

% Given the multiple modalities of the robot (e.g., navigation of the base, control of the manipulator, orientation of the cameras), a natural way to assist novice teleoperators is to automate some of these modalities in a manner that is complementary to the modality that the teleoperator controls.
% In this sub-activity, we specifically aim to develop a user assistive module for autonomously moving the mobile base in coordination with the manipulation motions under a novice teleoperator's direct control. 

% \paragraph*{Reward function formulation:} This activity aims to use inverse reinforcement learning (IRL) to infer the reward function determining when and how to move the mobile base to facilitate manipulation tasks being performed. We hypothetically consider reward functions with several motion coordination criteria based on the expert teleoperator's control. Given the common workspace of the nursing robot's arms, the mobile base is expected to pose the humanoid robot body according to: (1) the reachability of object(s) to be manipulated; (2) the average and minimal manipulability in the common workspace; (3) the clearance in the cluttered environment, including obstacles and the human partner; (4) the visibility for the teleoperator to view objects being manipulated from a convenient camera perspective; (5) minimal intervention to the manipulation tasks, because moving the mobile base will consume the mobile base battery and affect positioning accuracy. We will construct a reward function that assigns unknown weights to the variables that evaluate the above criteria, and use IRL to infer the weights from expert demonstrations of robot-mediated collaboration.

% \paragraph*{Shared-autonomous motion coordination module for assisting novice teleoperator:} Our previous system evaluation~\cite{Hauser_Li_TRINA:17} has demonstrated that direct teleoperation of a mobile humanoid nursing robot with many DOFs demands significant physical and mental effort. A teleoperator has to switch among controls for different components (arms, hands, mobile base, etc). However, input devices that only allow simultaneous control of a limited number of DOFs, such as a Geomagic Touch or gamepad, are still preferred over a whole-body exoskeleton due to cost, and over whole-body motion capture due to motion control inaccuracy.

% % Our user assistive module is built upon (1) the collaborative task knowledge and motion skills the robot has learned in Activity 1, and (2) the robot's capability of user intent inference developed in Activity 2.

% Thus, we propose to develop a shared-autonomous module to reduce the teleoperator's control effort. To illustrate the function of the proposed assistive module, consider the task of a nursing robot and human nurse collaboratively moving a patient transfer bed in a cluttered patient room. This task requires (1) coordinating the mobile base so the robot hands can grasp and push the patient transfer bed, and (2) coordinating the nursing robot motion with the human nurse to lead towards the direction of movement. In this collaboration, the assistive module assumes that: the human nurse has more task experience and perception/motion capability and thus will lead the task; and the novice user has knowledge of the high-level task goal (i.e., move the patient bed from point A to point B, avoid collision with environment), but may be unfamiliar with the mapping from the input device to the nursing robot, and thus cannot control motion accurately. With this assumption, the assistive module will make a tentative and conservative task division between the teleoperator and nursing robot, so that the teleoperator moves the nursing robot's end-effectors close to where he/she intends to grasp, while the robot will perform low-level vision-based motion planning to reach to grasp at the appropriate positions on the patient transfer bed. As the teleoperator controls the robot's hands to reach to grasp, the assistive module will move the mobile base toward the direction of the hand's reach. After the robot's hands have grasped, the assistive module will further utilized the kinematic redundancy of the robot arm to adjust the mobile base without affecting the grasping hands. This adjustment aims to optimize the weighted reward function.

% % which addresses the robot manipulatibility for pushing the patient bed, the clearance in the cluttered environment, and the advantages of camera vision. After both human and robot confirm their grasping, the assistive modual will be plan the mobile base in consistent with the moving direction the human nurse intends towards. When navigate in a cluttered patient room, both the human nurse and nursing robot needs to stop freqently and adjust where they stand and where their hands grasp. Based on where the human nurse grasps, the robot will utilize its knowledge of collaborative task decomposition to suggest the appropriate grasping region on the patient transfer bed to the novice teleoperator. As the teleoperator starts moving, the assistive module will determine where the novice aims to grasp based on the commanded motion direction and use the learned motion primitives to plan the reaching-to-grasp motion. 



% % \zhi{I will add learning inter-dependency of teleoperator and user's intent, using Bayesian network from demonstrated robot-mediated interaction}

% % \paragraph*{Learning teleoperation intent}

% % \zhi{Learning complex motion intent conveyed by the teleopertor's operation of input devices}

% % (1) teleoperation primitives; (2) Autonomous segmentation and classification; (3) learning correlation between teleoperator's action to manipulation effects 

% % \jie{Whatever you plan to write in this part, could potentially fits with the first subtask in task 2. Is it possible to merge? Also the current subtask 2.1 may need a bit more concrete details on what do we mean by talking about state and actions in IRL.}
% %-------------------------------------------------------------------------
% \subsubsection{Activity 2.4 --- Active intent detection for assisting novice user in collaborative manipulation}\label{sec:plan-intent-ActiveIntent}
% %-------------------------------------------------------------------------

% This sub-activity aims to develop a user assistive module to assist novice teleoperation in dexterous manipulation tasks in collaboration with the human nurse. Given that the novice teleoperator cannot produce accurate motions, we propose an active intent detection method to identify the object they intend to manipulate, and plan autonomous low-level motion. Often, the robot and nurse need to collaboratively pick and place a collection of objects. From a camera view, the assistive module will extract these objects and their affordance features, and mark where the teleoperator can potentially operate in an augmented camera view. Before the teleoperator starts moving, the assistive module initializes and prioritizes all the operable objects by their distance to the robot hands. As the teleoperator starts to move the robot, the assistive module adjusts the probability estimation to narrow the object the teleoperator intends to manipulate, and provides weak but sensible guiding forces to assist the teleoperator to reach and grasp the object with the highest probability. At this moment, the assistive module is still uncertain about the teleoperator's intent, but will use the teleoperator's reaction to the guiding force to resolve the ambiguity of intent. For example, after observing the teleoperator's motion direction, the assistive module might be uncertain as to whether the teleoperator's intent is to grasp a cup or a soft ball sitting very close it. To resolve the ambiguity, the assistive module will tentative guide the teleoperator towards the soft ball. The teleoperator will accept this suggestion if it is a correct guess, or it is an acceptable move that does not affect the task performance or high-level plan. If the teleoperator does not act against the suggestion, the assistive module will increase the assistive force and plan a low-level grasping motion. Otherwise, the assistive module will revise the probabilistic estimation of user intent, and switch to guide the teleoperator to operate on the cup. Before the teleoperator complies with the suggestion, the assistive module will keep suggesting operations on the objects and their affordance features in the direction the teleoperator moves towards. Objects that the human nurse intervenes to handle will be removed from the robot's consideration. The proposed online intent inference is computational feasible, since the number of objects and their affordance features are limited. This assistive strategy is developed based on the assumption that human teleoperators are not sensitive to optimal behavior: they can re-plan quickly, and are flexible to switch among policies of equivalent effect and performance. As long as satisfactory task performance can be achieved, the human prefers to act out of convenience, habit, and comfort, instead of pursuing optimality.

% % \todo{Jane: This activity aims to learn a reward function from an expert teleoperator for how to autonomously move the mobile based to facilitate the manipulation being performed (a typical IRL problem). This reward function may tradeoff among manipulability, clearance to obstacle, vision advantage, minimal intervention to reduce energy cost, alignment with the pushing direction. This activity leads to a user assistive module that allow a novice user focus on control of hands. The novice teleopertor can simply click a button and the base will be moved autonomously}. 

% %-------------------------------------------------------------------------
% \subsection{Activity 3 --- Implementation and experimental validation}\label{sec:plan-evaluation}
% %-------------------------------------------------------------------------

% % Developing intelligent shared-autonomous control modules to assist novice teleoperators in dexterous motion coordination and robot-mediated coordination
% \noindent
% % Goals: \textbf{Develop intelligent share-autonomous moduals to assist novice teleopertors to perform nursing tasks that involves (1) the coordination of dexterous manipulation and navigation, and (2) dexterous manipulation in collaboration with human patient and/or nurse; Implement the user assistive modual on the Tele-robotic Intelligent Nursing Assistant (TRINA) system; Validate the usability and efficacy of developed motion intelligence via user study. }

% % \begin{wrapfigure}{r}{3.0in}
% % \centering
% % % \vspace{-1em}
% % %\vspace{-1mm}
% %   \mbox{
% %   \includegraphics[width=1.0\linewidth]{fig//vive-baxter.jpg}
% %   }
% % \caption{(a) Visualization of Baxter workspace from Microsoft Kinect Camera to HTC Vive; (b) Operator holding HTC Vive controller slowly moves his hand from neutral position; (c) trainer moves robot hand manually to follow human trajectory; and (d) Baxter reaching the same configuration as human operator.}
% % \label{fig:vive}
% % \end{wrapfigure}
% \noindent

% %-------------------------------------------------------------------------
% \subsubsection{Activity 3.1 --- Implementation}\label{sec:plan-evaluation-implementation}
% %-------------------------------------------------------------------------
% This activity aims to implement methods proposed in Activities 1 and 2 on the Tele-robotic Intelligent Nursing Assistant (TRINA) system and evaluate their efficacy for the nursing tasks in~\fig{Tasks}. We select these frequently performed nursing tasks for (1) they involve complex task procedures and intimate human-robot collaborations and/or interactions, and (2) they are feasible under direct teleoperation given our robot platform's physical capability but take much more time than if performed by human nurse. Here we describe the demonstration tasks in detail as well as the metrics for system evaluation.
% % PM: Removed:, including the accuracy or log loss of the predicted task steps or task.  

% \paragraph*{Moving a patient transfer bed:} This task requires the teleoperated robot to collaborate with an on-site human nurse to move a patient transfer bed from the entrance of a simulated patient room to the side of a patient bed. This task aims to evaluate the robot's capability of learning and updating abstract task structures for collaboration, motion primitives and shared object affordances (Activity 1). From expert demonstrations, the robot develops its capabilities of (1) shared intent forecasting for high-level tasks, sub-tasks, and role assignments (Activity 2.1) and (2) teleoperator intent forecasting of subtasks from motion primitives through robot-mediated demonstrations (Activity 2.1). We will also investigate whether the robot can infer strategies for motion coordination and use shared-autonomous control module to assist a novice teleoperator in the coordination of manipulation and navigation (Activity 2.3). 

% \paragraph*{Organizing medical supplies and cleaning patient room:} In this task, the teleoperated robot will collaborate with an on-site human nurse to organize medical supplies scattered on a workstation into their containers. They will also work together to clean the patient room of debris, including dirty bedding and clothing from the patient's bed, as well as suction and urine containers. In this collaboration, the human nurse is responsible for task steps that require high dexterity (e.g., unplugging the suction container from the suction system) and involve intimate physical interactions with the patient (e.g., taking the dirty clothing off the patient's body). The robot is responsible for collecting or disposing of the debris and handling objects received from its human partner. This task aims to evaluate the robot's capacity to learn and update collaborative task structures and low-level motion skills (Activity 1) to render intuitive motions. The robot should learn preferred task-sharing based on explicit and implicit negotiation between the end user and expert teleoperator, and how to maximally utilize its capabilities (Activity 2.1 and 2.2) for task completion while adhering to the human's preferred task assignments. Through this task, we will also show how to use active intent detection to assist a novice user in dexterous manipulation (Activity 2.4).  % this is related to human subjects in the next task.

% \paragraph*{Preparing and serving food:} In this task, the nursing robot will load a food tray with containers for food, beverage and medicine cups, bring the prepared tray close to the patient's bed, and serve them to the patient in bed. This task aims to evaluate the robot's capability of learning and updating abstract task structure, motion primitives, and individual and shared object affordances (Activity 1). It also requires the robot to use fast action recognition and motion prediction during human-robot handover tasks (Activity 2.2). 
% It will be integrated with the navigation assistant module (Activity 2.3) to coordinate control of the mobile base for manipulation tasks and facilitate teleoperation.


% %-------------------------------------------------------------------------
% \subsubsection{Activity 3.2 --- Evaluation metrics for user study}\label{sec:plan-implementation-userstudy}
% %-------------------------------------------------------------------------
% % Learning-enabled assistive navigation and manipulation modules will be evaluated by both expert and novice teleoperators for independent tasks and by all human users (e.g., teleoperators, nurses, and patients) for collaborative tasks. 
% %The proposed research involves human subjects in both robot learning and system evaluation.
% This activity will evaluate how much the assistive module can improve the experience of teleoperation and shorten the learning curve for novice users. Given the nursing tasks shown in~\fig{Tasks}, we will first teach experienced nurses to control the robot in direct teleoperation, and then collect the robot-mediated collaborations with the human partners in a simulated patient room. To record the nurses' learning curve, we will repeat the cycle of teleoperation training and task performing, until the nurse under training can successfully teleoperate the robot to finish the collaborative task. For comparative evaluation, we will teach experienced nurses new to the nursing robot to control the robot with the developed assistive modules, and perform the same set of tasks. We will compare the learning curve of the teleoperator nurses with and without the assistive modules, to validate the effects of learning-enabled components in TRINA. 

% This activity will provide a more human-centric evaluation of the capability of lifelong learning and intent inferences. In addition, it will compare the user experience under shared-autonomous control and direct teleoperation. During the nurse's learning process, we will observe changes in task completion time, success rate, error type and frequency. We will use the NASA Task Load Index (NASA-TLX) to evaluate the experience of expert and novice teleoperators and end users, to measure their physical, mental, and temporal demands as well as effort and frustration in robot teaching and use. Through questionnaires, we will investigate: (1) Does the autonomous motion coordination comply with the manipulation they intend to perform? (2) Does the robot adapt quickly to their preference of task operation? (3) Is the robot's behavior natural to the on-site human partner? (4) Do the assistive modules facilitate learning the motion correspondence between the robot and operator, and therefore reduce operator training effort? Through the user study, we aim to answer the general research question of what level of motion intelligence a mediating robot should develop to support a beneficial level of shared-autonomy.




% %-------------------------------------------------------------------------
% \section{Broader Impacts}\label{sec:impact}
% %-------------------------------------------------------------------------

% % Broader impacts may be accomplished through the research itself, through the activities that are directly related to specific research projects, or through activities that are supported by, but are complementary to, the project. NSF values the advancement of scientific knowledge and activities that contribute to achievement of societally relevant outcomes. Such outcomes include, but are not limited to: full participation of women, persons with disabilities, and underrepresented minorities in science, technology, engineering, and mathematics (STEM); improved STEM education and educator development at any level; increased public scientific literacy and public engagement with science and technology; improved well-being of individuals in society; development of a diverse, globally competitive STEM workforce; increased partnerships between academia, industry, and others; improved national security; increased economic competitiveness of the United States; and enhanced infrastructure for research and education.

% \paragraph*{Broader Technological and Societal Impacts:} 

% This project will have broader impacts on a wide range of medical, industrial, and social service tasks involving mobile humanoid robots. Our research effort aims to remove major barriers that prevent robots from integrating into human society as capable and socially acceptable peers, and provides a paradigm for transferring motion skills between human and robots. By improving the usability of dexterous robotic manipulators under direct teleoperation and shared-autonomous control, this project may also lead to improved availability of healthcare, industrial and social service labor, and provide surrogates for military and medical personnel for tedious, repetitive, and dangerous tasks. It will lead to affordable robotic solutions for hospital- and home-care that can provide long-term assistance to aging and disabled populations.

% \paragraph*{Integrating Research with Education:} 

% The proposed research will be integrated with undergraduate and graduate courses offered by the WPI robotics engineering program (RBE) and Computer Science at UIC. The humanoid nursing robot will serve as infrastructure for both research and education. PI Li will develop a course on physical human-robot interaction for graduate and senior undergraduate students. Students will actively participate in the research via their course projects. Toward the goals of the proposed research, the PI is also advising Major Qualifying Project teams working on: (1) arm-hand and bimanual coordination, (2) developing multi-fingered grippers for dexterous manipulation, and (3) developing an intuitive motion control interface to reduce training effort for novice teleoperators. 


% \paragraph*{Diversity and Outreach Activities:}
% Beside education activities at WPI, this research will engage research and teaching faculty and students from local nursing schools, and promote education around tele-robotic approaches for healthcare. It aims to encourage woman students in the nursing school to actively participate in the development of the humanoid nursing robot by providing valuable insights and experience from their nursing education and practice. It will also prepare students of the nursing school for a new generation of medical robotic technologies that will be widely available in near future. To promote K-12 education, the humanoid nursing robot will serve as an educational platform through the WPI First Robotics Competition, and will be open to the general public at the WPI TouchTomorrow robotics event. PI Li and Fu's lab will reach out to local underprivileged high schools and organize free robotics workshops and open lab events for their students. This research will also contribute to the Robotics Education program between WPI and the World Bank for Sub-Saharan Africa, to promote STEM practice and engagement amongst high school students using robotics. 
% PI Ziebart has a strong track record of involving undergraduates from groups that are underrepresented in computer science (women and African-Americans) in NSF-supported research projects and will continue to do so through the Research Experiences for Undergraduates (REU) program.

% \paragraph*{Broader Dissemination:}
% This research will produce academic journal and conference publications and software packages released on the World Wide Web. Software packages that implement the proposed motion control methods will be developed for compatibility with the Robot Operating System (ROS). Based on the research, the PIs plan to organize a workshop at an international robotics conference (ICRA, IROS) on robot-mediated interaction. Course Materials (lecture notes, homework, project design, etc.) based on the proposed research will also be shared on the PIs' research websites. Overall, this research will improve the well-being of individuals in society, contribute to the development of a diverse, globally competitive STEM workforce, and increase the economic competitiveness of the United States.

% % In 2017 Fall, the PI will teach a graduate course of synergy of human and robotic systems at WPI. This course covers the design and motion control of human-compatible and human-like robots, and will include the proposed Task 1 and 2 as course projects. Two Major Qualifying Project (MQP) teams the PI advises in 2017-2018 academic year will work on vision-based shared-autonomous control of a humanoid manufacturing robot, and the design and motion control of a dexterous robotic hand for precise manipulation. This research will actively engage K-12 students through WPI robotic events open to the general public, such as TouchTomorrow and FirstRobotics Competition. 

% %-------------------------------------------------------------------------
% \section{Results from Previous NSF Support}\label{sec:priorNSF}
% %-------------------------------------------------------------------------

% PI \textbf{Zhi Li} has not received prior NSF support.

% \noindent
% \textbf{B. Ziebart} was a sub-contractee on NRI-\#1227495 (\$509,409,
% 10/2012-9/2017),``Collaborative Research:
% Purposeful Prediction: Co-robot Interaction via Understanding Intent
% and Goals,'' is a PI on RI-\#1526379 (\$500,000, 09/2015-8/2018),
% ``Robust Optimization of Loss Functions with Application to
% Active Learning'') and is a PI on III-\#1514126 (\$636,454, 09/2015-08/2018)
% ``Collaborative Research: Computational tools for extracting individual,
% dyadic, and network behavior from remotely sensed data.''
% The {\bf Intellectual Merits} for these awards include: the
% creation of a framework enabling autonomous robots to anticipate and adapt to the
% activities of their human co-workers (NRI); developing techniques that
% better align learning objectives with application performance measures
% in classification settings (RI);
% and developing methods for making sense of massive amounts of social animal
% sensor data (III).
% The {\bf Broader Impacts}
% include the significant efficiency improvements in small- and
% medium-scale manufacturing that these improved interaction technologies
% will facilitate (NRI); improving the application of machine learning to a
% range of domains with non-convex loss measures
% (RI); and effectively multiplying the expertise
% of field biologists when interpreting sensor data (III).
% To date, these projects have produced
% a total of ten conference publications and six workshop publications
% covering the topics of
% inverse optimal control \cite{asif2013inferring,monfort2013predictive,
% byravan2014layered,monfort2015intent,chen2015predictive,
% byravan2015graph,monfort2015softstar,chen2015imitation,chen2016adversarial},
% learning under covariate shift/active learning
% \cite{liu2014robust,liu2015shift,liu2015addressing,chen2016robust},
% inductively optimizing multivariate losses \cite{wang2015adversarial},
% sequence prediction \cite{li2016adversarial}, and learning to interact
% \cite{behpour2015minimax}.

% \noindent
% \textbf{J. Fu} is the Co-PI of
% \underline{CMMI-1728412}: \textbf{Intelligent Soft Robot Mobility in
%   the Real World,} (PI: C. Onal, $200,000$ for Co-PI J. Fu, 08/15/2017
% - 07/31/2020). \textbf{Intellectual Merit:} The proposed research will
% advance soft robot intelligence and mobility in uncertain real-world
% environments that cannot be safety traversed using rigid
% systems.  \textbf{Broader Impact:} This research will pave the way for
% soft robots to be utilized at farms, homes, manufacturing sites, and
% search and rescue scenarios, and impact education in
% robotics.

% % %-------------------------------------------------------------------------
% % \section*{APPENDIX}
% % %-------------------------------------------------------------------------

% % Appendixes should appear before the acknowledgment.

% % %-------------------------------------------------------------------------
% % \section*{ACKNOWLEDGMENT}
% % %-------------------------------------------------------------------------
% \pagebreak
% \setcounter{page}{1}
% \setcounter{section}{0}
% \input{sup_collab.tex}
% \pagebreak
% \setcounter{section}{0}
% \input{sup_subject.tex}
% \pagebreak
% \setcounter{section}{0}
% \input{sup_data.tex}
% \pagebreak
% \setcounter{section}{0}
% \input{sup_facility.tex}

% \pagebreak
% \setcounter{page}{1}
% \bibliographystyle{IEEEtran}
% \bibliography{./ref}

\end{document}
